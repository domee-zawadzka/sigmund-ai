[
  {
    "content": "# datamatrix.series\n\ntitle: datamatrix.series\n\n\nThis module is typically imported as `srs` for brevity:\n\n```python\nfrom datamatrix import series as srs\n```\n\n\n\n[TOC]\n\n## What are series?\n\nA `SeriesColumn` is a column with a depth; that is, each cell contains multiple values. Data of this kind is very common. For example, imagine a psychology experiment in which participants see positive or negative pictures, while their brain activity is recorded using electroencephalography (EEG). Here, picture type (positive or negative) is a single value that could be stored in a normal table. But EEG activity is a continuous signal, and could be stored as `SeriesColumn`.\n\nA `SeriesColumn` is identical to a `MultiDimensionalColumn` with a shape of length 1. Therefore, all functions in the [`multidimensional` module](%url:multidimensional) can also be applied to `SeriesColumn`s.\n\nFor more information, see:\n\n- <https://pythontutorials.eu/numerical/time-series/>\n\n<div class=\" YAMLDoc\" id=\"\" markdown=\"1\">\n\n \n\n<div class=\"FunctionDoc YAMLDoc\" id=\"baseline\" markdown=\"1\">\n\n## function __baseline__\\(series, baseline, bl\\_start=-100, bl\\_end=None, reduce\\_fnc=None, method=u'subtractive'\\)\n\nApplies a baseline to a signal\n\n__Example:__\n\n%--\npython: |\n import numpy as np\n from matplotlib import pyplot as plt\n from datamatrix import DataMatrix, SeriesColumn, series as srs\n\n LENGTH = 5 # Number of rows\n DEPTH = 10 # Depth (or length) of SeriesColumns\n\n sinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n\n dm = DataMatrix(length=LENGTH)\n # First create five identical rows with a sinewave\n dm.y = SeriesColumn(depth=DEPTH)\n dm.y.setallrows(sinewave)\n # Add a random offset to the Y values\n dm.y += np.random.random(LENGTH)\n # And also a bit of random jitter\n dm.y += .2*np.random.random( (LENGTH, DEPTH) )\n # Baseline-correct the traces, This will remove the vertical\n # offset\n dm.y2 = srs.baseline(dm.y, dm.y, bl_start=0, bl_end=10)\n\n plt.clf()\n plt.subplot(121)\n plt.title('Original')\n plt.plot(dm.y.plottable)\n plt.subplot(122)\n plt.title('Baseline corrected')\n plt.plot(dm.y2.plottable)\n plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- The signal to apply a baseline to.\n\t- Type: SeriesColumn\n- `baseline` -- The signal to use as a baseline to.\n\t- Type: SeriesColumn\n\n__Keywords:__\n\n- `bl_start` -- The start of the window from `baseline` to use.\n\t- Type: int\n\t- Default: -100\n- `bl_end` -- The end of the window from `baseline` to use, or None to go to the end.\n\t- Type: int, None\n\t- Default: None\n- `reduce_fnc` -- The function to reduce the baseline epoch to a single value. If None, np.nanmedian() is used.\n\t- Type: FunctionType, None\n\t- Default: None\n- `method` -- Specifies whether divisive or subtractive baseline\ncorrection should be used. (*Changed in v0.7.0: subtractive\nis now the default*)\n\t- Type: str\n\t- Default: 'subtractive'\n\n__Returns:__\n\nA baseline-correct version of the signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"blinkreconstruct\" markdown=\"1\">\n\n## function __blinkreconstruct__\\(series, vt=5, vt\\_start=10, vt\\_end=5, maxdur=500, margin=10, smooth\\_winlen=21, std\\_thr=3, gap\\_margin=20, gap\\_vt=10, mode=u'original'\\)\n\nReconstructs pupil size during blinks. This algorithm has been designed\nand tested largely with the EyeLink 1000 eye tracker.\n\n*Version note:* As of 0.13.0, an advanced algorithm has been\nintroduced, wich can be specified through the `mode` keyword. The\nadvanced algorithm is recommended for new analyses, and will be made\nthe default in future releases.\n\n*Version note:* As of 1.0.5 the advanced algorithm has been updated\nwith a [bugfix](https://github.com/open-cogsci/datamatrix/pull/18) and\nthe end of a blink is defined as the moment where the velocity drops\nto 1% of the velocity standard deviation, as opposed to 0.\n\n__Source:__\n\n- Mathot, S., & Vilotijevi\u0107, A. (2022). Methods in cognitive \n  pupillometry: Design, preprocessing, and statitical analysis.\n  *Behavior Research Methods*.\n  <https://doi.org/10.3758/s13428-022-01957-7>\n\n__Arguments:__\n\n- `series` -- A signal to reconstruct.\n\t- Type: SeriesColumn\n\n__Keywords:__\n\n- `vt` -- A pupil-velocity threshold for blink detection. Lower tresholds more easily trigger blinks. This argument only applies to 'original' mode.\n\t- Type: int, float\n\t- Default: 5\n- `vt_start` -- A pupil-velocity threshold for detecting the onset of a blink. Lower tresholds more easily trigger blinks. This argument only applies to 'advanced' mode.\n\t- Type: int, float\n\t- Default: 10\n- `vt_end` -- A pupil-velocity threshold for detecting the offset of a blink. Lower tresholds more easily trigger blinks. This argument only applies to 'advanced' mode.\n\t- Type: int, float\n\t- Default: 5\n- `maxdur` -- The maximum duration (in samples) for a blink. Longer blinks are not reconstructed.\n\t- Type: int\n\t- Default: 500\n- `margin` -- The margin to take around missing data that is reconstructed.\n\t- Type: int\n\t- Default: 10\n- `smooth_winlen` -- The window length for a hanning window that is used to smooth the velocity profile.\n\t- Type: int\n\t- Default: 21\n- `std_thr` -- A standard-deviation threshold for when data should be considered invalid.\n\t- Type: float, int\n\t- Default: 3\n- `gap_margin` -- The margin to take around missing data that is not reconstructed. Only applies to advanced mode.\n\t- Type: int\n\t- Default: 20\n- `gap_vt` -- A pupil-velocity threshold for detection of invalid data. Lower tresholds mean more data marked as invalid. Only applies to advanced mode.\n\t- Type: int, float\n\t- Default: 10\n- `mode` -- The algorithm to be used for blink reconstruction. Should be 'original' or 'advanced'. An advanced algorith was introduced in v0.13., and should be used for new analysis. The original algorithm is still the default for backwards compatibility.\n\t- Type: str\n\t- Default: 'original'\n\n__Returns:__\n\nA reconstructed singal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"concatenate\" markdown=\"1\">\n\n## function __concatenate__\\(\\*series\\)\n\nConcatenates multiple series such that a new series is created with a\ndepth that is equal to the sum of the depths of all input series.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import series as srs\n\n dm = DataMatrix(length=1)\n dm.s1 = SeriesColumn(depth=3)\n dm.s1[:] = 1,2,3\n dm.s2 = SeriesColumn(depth=3)\n dm.s2[:] = 3,2,1\n dm.s = srs.concatenate(dm.s1, dm.s2)\n print(dm.s)\n--%\n\n__Argument list:__\n\n- `*series`: A list of series.\n\n__Returns:__\n\nA new series.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"downsample\" markdown=\"1\">\n\n## function __downsample__\\(series, by, fnc=<function nanmean at 0x7f31341e1090>\\)\n\nDownsamples a series by a factor, so that it becomes 'by' times\nshorter. The depth of the downsampled series is the highest multiple of\nthe depth of the original series divided by 'by'. For example,\ndownsampling a series with a depth of 10 by 3 results in a depth of 3.\n\n__Example:__\n\n%--\npython: |\n import numpy as np\n from matplotlib import pyplot as plt\n from datamatrix import DataMatrix, SeriesColumn, series as srs\n\n LENGTH = 1 # Number of rows\n DEPTH = 100 # Depth (or length) of SeriesColumns\n\n sinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n\n dm = DataMatrix(length=LENGTH)\n dm.y = SeriesColumn(depth=DEPTH)\n dm.y.setallrows(sinewave)\n dm.y2 = srs.downsample(dm.y, by=10)\n\n plt.clf()\n plt.subplot(121)\n plt.title('Original')\n plt.plot(dm.y.plottable, 'o-')\n plt.subplot(122)\n plt.title('Downsampled')\n plt.plot(dm.y2.plottable, 'o-')\n plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- No description\n- `by` -- The downsampling factor.\n\t- Type: int\n\n__Keywords:__\n\n- `fnc` -- The function to average the samples that are combined into 1 value. Typically an average or a median.\n\t- Type: callable\n\t- Default: <function nanmean at 0x7f31341e1090>\n\n__Returns:__\n\nA downsampled series.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"endlock\" markdown=\"1\">\n\n## function __endlock__\\(series\\)\n\nLocks a series to the end, so that any nan-values that were at the end\nare moved to the start.\n\n__Example:__\n\n%--\npython: |\n import numpy as np\n from matplotlib import pyplot as plt\n from datamatrix import DataMatrix, SeriesColumn, series as srs\n\n LENGTH = 5 # Number of rows\n DEPTH = 10 # Depth (or length) of SeriesColumns\n\n sinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n\n dm = DataMatrix(length=LENGTH)\n # First create five identical rows with a sinewave\n dm.y = SeriesColumn(depth=DEPTH)\n dm.y.setallrows(sinewave)\n # Add a random offset to the Y values\n dm.y += np.random.random(LENGTH)\n # Set some observations at the end to nan\n for i, row in enumerate(dm):\n    row.y[-i:] = np.nan\n # Lock the degraded traces to the end, so that all nans\n # now come at the start of the trace\n dm.y2 = srs.endlock(dm.y)\n\n plt.clf()\n plt.subplot(121)\n plt.title('Original (nans at end)')\n plt.plot(dm.y.plottable)\n plt.subplot(122)\n plt.title('Endlocked (nans at start)')\n plt.plot(dm.y2.plottable)\n plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- The signal to end-lock.\n\t- Type: SeriesColumn\n\n__Returns:__\n\nAn end-locked signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"fft\" markdown=\"1\">\n\n## function __fft__\\(series, truncate=True\\)\n\n*New in v0.9.2*\n\nPerforms a fast-fourrier transform (FFT) for the signal. For more\ninformation, see [`numpy.fft`](https://docs.scipy.org/doc/numpy/reference/routines.fft.html#module-numpy.fft).\n\n__Example:__\n\n%--\npython: |\n import numpy as np\n from matplotlib import pyplot as plt\n from datamatrix import DataMatrix, SeriesColumn, series as srs\n\n LENGTH = 3\n DEPTH = 200\n\n # Create one fast oscillation, and two combined fast and slow\n # oscillations\n dm = DataMatrix(length=LENGTH)\n dm.s = SeriesColumn(depth=DEPTH)\n dm.s[0] = np.sin(np.linspace(0, 150 * np.pi, DEPTH))\n dm.s[1] = np.sin(np.linspace(0, 75 * np.pi, DEPTH))\n dm.s[2] = np.sin(np.linspace(0, 10 * np.pi, DEPTH))\n dm.f = srs.fft(dm.s)\n\n # Plot the original signal\n plt.clf()\n plt.subplot(121)\n plt.title('Original')\n plt.plot(dm.s[0])\n plt.plot(dm.s[1])\n plt.plot(dm.s[2])\n plt.subplot(122)\n # And the filtered signal!\n plt.title('FFT')\n plt.plot(dm.f[0])\n plt.plot(dm.f[1])\n plt.plot(dm.f[2])\n plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- A signal to determine the FFT for.\n\t- Type: SeriesColumn\n\n__Keywords:__\n\n- `truncate` -- FFT series of real signals are symmetric. The `truncate` keyword indicates whether the last (symmetric) part of the FFT should be removed.\n\t- Type: bool\n\t- Default: True\n\n__Returns:__\n\nThe FFT of the signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"filter_bandpass\" markdown=\"1\">\n\n## function __filter\\_bandpass__\\(series, freq\\_range, order=2, sampling\\_freq=None\\)\n\n*New in v0.9.2*\n\n*Changed in v0.11.0: added `sampling_freq` argument* \n\nApplies a Butterworth bandpass-pass filter to the signal.\n\nFor more information, see [`scipy.signal`](https://docs.scipy.org/doc/scipy/reference/signal.html).\n\n__Example:__\n\n%--\npython: |\n    import numpy as np\n    from matplotlib import pyplot as plt\n    from datamatrix import DataMatrix, SeriesColumn, series as srs\n    \n    LENGTH = 3\n    DEPTH = 100\n    SAMPLING_FREQ = 100\n    \n    # Create one fast oscillation, and two combined fast and slow\n    # oscillations\n    dm = DataMatrix(length=LENGTH)\n    dm.s = SeriesColumn(depth=DEPTH)\n    dm.s[0] = np.sin(np.linspace(0, 20 * np.pi, DEPTH))  # 10 Hz\n    dm.s[1] = np.sin(np.linspace(0, 10 * np.pi, DEPTH)) + dm.s[0]  # 5 Hz\n    dm.s[2] = np.cos(np.linspace(0, 2 * np.pi, DEPTH)) + dm.s[0]  # 1 Hz\n    dm.f = srs.filter_bandpass(dm.s, freq_range=(4, 6), sampling_freq=SAMPLING_FREQ)\n    \n    # Plot the original signal\n    plt.clf()\n    plt.subplot(121)\n    plt.title('Original')\n    plt.plot(dm.s[0])\n    plt.plot(dm.s[1])\n    plt.plot(dm.s[2])\n    plt.subplot(122)\n    # And the filtered signal!\n    plt.title('Bandpass')\n    plt.plot(dm.f[0])\n    plt.plot(dm.f[1])\n    plt.plot(dm.f[2])\n    plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- A signal to filter.\n\t- Type: SeriesColumn\n- `freq_range` -- A `(min_freq, max_freq)` tuple.\n\t- Type: tuple\n\n__Keywords:__\n\n- `order` -- The order of the filter.\n\t- Type: int\n\t- Default: 2\n- `sampling_freq` -- The sampling frequence of the signal, or `None` to use the scipy default of 2 half-cycles per sample.\n\t- Type: int, None\n\t- Default: None\n\n__Returns:__\n\nThe filtered signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"filter_highpass\" markdown=\"1\">\n\n## function __filter\\_highpass__\\(series, freq\\_min, order=2, sampling\\_freq=None\\)\n\n*New in v0.9.2*\n\n*Changed in v0.11.0: added `sampling_freq` argument* \n\nApplies a Butterworth highpass-pass filter to the signal.\n\nFor more information, see [`scipy.signal`](https://docs.scipy.org/doc/scipy/reference/signal.html).\n\n__Example:__\n\n%--\npython: |\n    import numpy as np\n    from matplotlib import pyplot as plt\n    from datamatrix import DataMatrix, SeriesColumn, series as srs\n    \n    LENGTH = 3\n    DEPTH = 100\n    SAMPLING_FREQ = 100\n    \n    # Create one fast oscillation, and two combined fast and slow\n    # oscillations\n    dm = DataMatrix(length=LENGTH)\n    dm.s = SeriesColumn(depth=DEPTH)\n    dm.s[0] = np.sin(np.linspace(0, 20 * np.pi, DEPTH))  # 10 Hz\n    dm.s[1] = np.sin(np.linspace(0, 2 * np.pi, DEPTH)) + dm.s[0]  # 1 Hz\n    dm.s[2] = np.cos(np.linspace(0, 2 * np.pi, DEPTH)) + dm.s[0]  # 1 Hz\n    dm.f = srs.filter_highpass(dm.s, freq_min=3, sampling_freq=SAMPLING_FREQ)\n    \n    # Plot the original signal\n    plt.clf()\n    plt.subplot(121)\n    plt.title('Original')\n    plt.plot(dm.s[0])\n    plt.plot(dm.s[1])\n    plt.plot(dm.s[2])\n    plt.subplot(122)\n    # And the filtered signal!\n    plt.title('Highpass')\n    plt.plot(dm.f[0])\n    plt.plot(dm.f[1])\n    plt.plot(dm.f[2])\n    plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- A signal to filter.\n\t- Type: SeriesColumn\n- `freq_min` -- The minimum filter frequency.\n\t- Type: int\n\n__Keywords:__\n\n- `order` -- The order of the filter.\n\t- Type: int\n\t- Default: 2\n- `sampling_freq` -- The sampling frequence of the signal, or `None` to use the scipy default of 2 half-cycles per sample.\n\t- Type: int, None\n\t- Default: None\n\n__Returns:__\n\nThe filtered signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"filter_lowpass\" markdown=\"1\">\n\n## function __filter\\_lowpass__\\(series, freq\\_max, order=2, sampling\\_freq=None\\)\n\n*New in v0.9.2*\n\n*Changed in v0.11.0: added `sampling_freq` argument* \n\nApplies a Butterworth low-pass filter to the signal.\n\nFor more information, see [`scipy.signal`](https://docs.scipy.org/doc/scipy/reference/signal.html).\n\n__Example:__\n\n%--\npython: |\n    import numpy as np\n    from matplotlib import pyplot as plt\n    from datamatrix import DataMatrix, SeriesColumn, series as srs\n    \n    LENGTH = 3\n    DEPTH = 100\n    SAMPLING_FREQ = 100\n    \n    # Create one fast oscillation, and two combined fast and slow\n    # oscillations\n    dm = DataMatrix(length=LENGTH)\n    dm.s = SeriesColumn(depth=DEPTH)\n    dm.s[0] = np.sin(np.linspace(0, 20 * np.pi, DEPTH))  # 10 Hz\n    dm.s[1] = np.sin(np.linspace(0, 2 * np.pi, DEPTH)) + dm.s[0]  # 1 Hz\n    dm.s[2] = np.cos(np.linspace(0, 2 * np.pi, DEPTH)) + dm.s[0]  # 1 Hz\n    dm.f = srs.filter_lowpass(dm.s, freq_max=3, sampling_freq=SAMPLING_FREQ)\n    \n    # Plot the original signal\n    plt.clf()\n    plt.subplot(121)\n    plt.title('Original')\n    plt.plot(dm.s[0])\n    plt.plot(dm.s[1])\n    plt.plot(dm.s[2])\n    plt.subplot(122)\n    # And the filtered signal!\n    plt.title('Lowpass')\n    plt.plot(dm.f[0])\n    plt.plot(dm.f[1])\n    plt.plot(dm.f[2])\n    plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- A signal to filter.\n\t- Type: SeriesColumn\n- `freq_max` -- The maximum filter frequency.\n\t- Type: int\n\n__Keywords:__\n\n- `order` -- The order of the filter.\n\t- Type: int\n\t- Default: 2\n- `sampling_freq` -- The sampling frequence of the signal, or `None` to use the scipy default of 2 half-cycles per sample.\n\t- Type: int, None\n\t- Default: None\n\n__Returns:__\n\nThe filtered signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"first_occurrence\" markdown=\"1\">\n\n## function __first\\_occurrence__\\(series, value, equal=True\\)\n\nFinds the first occurence of a value for each row of a series column\nand returns the result as a float column of sample indices.\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, SeriesColumn, NAN, series as srs\n\n dm = DataMatrix(length=3)\n dm.s = SeriesColumn(depth=3)\n dm.s[0] = 1, 2, 3\n dm.s[1] = 1, 2, NAN\n dm.s[2] = NAN, NAN, NAN\n dm.first_nan = srs.first_occurrence(dm.s, value=NAN)\n dm.first_non_1 = srs.first_occurrence(dm.s, value=1, equal=False)\n print(dm)\n--%\n\n__Arguments:__\n\n- `series` -- The series column to search\n\t- Type: SeriesColumn\n- `value` -- The value to find in the series column. If `value` is a\nsequence, which has to be of the same length as the series,\nthen each row is searched for the value indicated by the\ncorresponding value in `value`.\n\t- Type: float, int, Sequence\n\n__Keywords:__\n\n- `equal` -- If `True`, the index of the first matching sample is returned. If `False`, the index of the first non-matching sample is returned.\n\t- Type: bool\n\t- Default: True\n\n__Returns:__\n\nA float column with sample indices or `NAN` for cells in which there was no match (or no mismatch if `equal=False`).\n\n- Type: FloatColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"interpolate\" markdown=\"1\">\n\n## function __interpolate__\\(series\\)\n\nLinearly interpolates missing (`nan`) data.\n\n__Example:__\n\n%--\npython: |\n import numpy as np\n from matplotlib import pyplot as plt\n from datamatrix import DataMatrix, SeriesColumn, series as srs\n\n LENGTH = 1 # Number of rows\n DEPTH = 100 # Depth (or length) of SeriesColumns\n MISSING = 50 # Nr of missing samples\n\n # Create a sine wave with missing data\n sinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n sinewave[np.random.choice(np.arange(DEPTH), MISSING)] = np.nan\n # And turns this into a DataMatrix\n dm = DataMatrix(length=LENGTH)\n dm.y = SeriesColumn(depth=DEPTH)\n dm.y = sinewave\n # Now interpolate the missing data!\n dm.i = srs.interpolate(dm.y)\n\n # And plot the original data as circles and the interpolated data as\n # dotted lines\n plt.clf()\n plt.plot(dm.i.plottable, ':')\n plt.plot(dm.y.plottable, 'o')\n plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- A signal to interpolate.\n\t- Type: SeriesColumn\n\n__Returns:__\n\nThe interpolated signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"last_occurrence\" markdown=\"1\">\n\n## function __last\\_occurrence__\\(series, value, equal=True\\)\n\nFinds the last occurence of a value for each row of a series column\nand returns the result as a float column of sample indices.\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, SeriesColumn, NAN\n\n dm = DataMatrix(length=3)\n dm.s = SeriesColumn(depth=3)\n dm.s[0] = 1, 2, 3\n dm.s[1] = 1, 2, NAN\n dm.s[2] = NAN, NAN, NAN\n dm.last_nan = srs.last_occurrence(dm.s, value=NAN)\n dm.last_non_1 = srs.last_occurrence(dm.s, value=1, equal=False)\n print(dm)\n--%\n\n__Arguments:__\n\n- `series` -- The series column to search\n\t- Type: SeriesColumn\n- `value` -- The value to find in the series column. If `value` is a\nsequence, which has to be of the same length as the series,\nthen each row is searched for the value indicated by the\ncorresponding value in `value`.\n\t- Type: float, int, Sequence\n\n__Keywords:__\n\n- `equal` -- If `True`, the index of the last matching sample is returned. If `False`, the index of the last non-matching sample is returned.\n\t- Type: bool\n\t- Default: True\n\n__Returns:__\n\nA float column with sample indices or `NAN` for cells in which there was no match (or no mismatch if `equal=False`).\n\n- Type: FloatColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"lock\" markdown=\"1\">\n\n## function __lock__\\(series, lock\\)\n\nShifts each row from a series by a certain number of steps along its\ndepth. This is useful to lock, or align, a series based on a sequence\nof values.\n\n__Example:__\n\n%--\npython: |\n import numpy as np\n from matplotlib import pyplot as plt\n from datamatrix import DataMatrix, SeriesColumn, series as srs\n\n LENGTH = 5 # Number of rows\n DEPTH = 10 # Depth (or length) of SeriesColumns\n\n dm = DataMatrix(length=LENGTH)\n # First create five traces with a partial cosinewave. Each row is\n # offset slightly on the x and y axes\n dm.y = SeriesColumn(depth=DEPTH)\n dm.x_offset = -1\n dm.y_offset = -1\n for row in dm:\n    row.x_offset = np.random.randint(0, DEPTH)\n    row.y_offset = np.random.random()\n    row.y = np.roll(\n        np.cos(np.linspace(0, np.pi, DEPTH)),\n        row.x_offset\n    ) + row.y_offset\n # Now use the x offset to lock the traces to the 0 point of the\n # cosine, i.e. to their peaks.\n dm.y2, zero_point = srs.lock(dm.y, lock=dm.x_offset)\n\n plt.clf()\n plt.subplot(121)\n plt.title('Original')\n plt.plot(dm.y.plottable)\n plt.subplot(122)\n plt.title('Locked to peak')\n plt.plot(dm.y2.plottable)\n plt.axvline(zero_point, color='black', linestyle=':')\n plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- The signal to lock.\n\t- Type: SeriesColumn\n- `lock` -- A sequence of lock values with the same length as the Series. This can be a column, a list, a numpy array, etc.\n\n__Returns:__\n\nA `(series, zero_point)` tuple, in which `series` is a `SeriesColumn` and `zero_point` is the zero point to which the signal has been locked.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"normalize_time\" markdown=\"1\">\n\n## function __normalize\\_time__\\(dataseries, timeseries\\)\n\n*New in v0.7.0*\n\nCreates a new series in which a series of timestamps (`timeseries`) is\nused as the indices for a series of data point (`dataseries`). This is\nuseful, for example, if you have a series of measurements and a\nseparate series of timestamps, and you want to combine the two.\n\nThe resulting series will generally contain a lot of `nan` values,\nwhich you can interpolate with `interpolate()`.\n\n__Example:__\n\n%--\npython: |\n from matplotlib import pyplot as plt\n from datamatrix import DataMatrix, SeriesColumn, series as srs, NAN\n\n # Create a DataMatrix with one series column that contains samples\n # and one series column that contains timestamps.\n dm = DataMatrix(length=2)\n dm.samples = SeriesColumn(depth=3)\n dm.time = SeriesColumn(depth=3)\n dm.samples[0] = 3, 1, 2\n dm.time[0]    = 1, 2, 3\n dm.samples[1] = 1, 3, 2\n dm.time[1]    = 0, 5, 10\n # Create a normalized column with samples spread out according to\n # the timestamps, and also create an interpolate version of this\n # column for smooth plotting.\n dm.normalized = srs.normalize_time(\n    dataseries=dm.samples,\n    timeseries=dm.time\n )\n dm.interpolated = srs.interpolate(dm.normalized)\n # And plot!\n plt.clf()\n plt.plot(dm.normalized.plottable, 'o')\n plt.plot(dm.interpolated.plottable, ':')\n plt.xlabel('Time')\n plt.ylabel('Data')\n plt.show()\n--%\n\n__Arguments:__\n\n- `dataseries` -- A column with datapoints.\n\t- Type: SeriesColumn\n- `timeseries` -- A column with timestamps. This should be an increasing list of the same depth as `dataseries`. NAN values are allowed, but only at the end.\n\t- Type: SeriesColumn\n\n__Returns:__\n\nA new series in which the data points are spread according to the timestamps.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"roll\" markdown=\"1\">\n\n## function __roll__\\(series, shift\\)\n\nRolls (or shifts) the elements along the depth of the series. Elements\nthat run off the last position are re-introduced at the first position\nand vice versa.\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, SeriesColumn, series as srs\n\n dm = DataMatrix(length=3)\n dm.s = SeriesColumn(depth=4)\n dm.s = [[1, 2, 3, 4],\n         [10, 20, 30, 40],\n         [100, 200, 300, 400]]\n dm.t = srs.roll(dm.s, shift=1)\n dm.u = srs.roll(dm.s, shift=[1, 0, -1])\n print(dm)\n--%\n\n__Arguments:__\n\n- `series` -- The series column to roll\n\t- Type: SeriesColumn\n- `shift` -- The number of places to roll by. If `shift` is an `int`, each row is shifted by the same amount. If `shift` is a sequence, which has to be of the same length as the series, then each row is shifted by the amounted indicated by the corresponding value in `shift`.\n\t- Type: int, Sequence\n\n__Returns:__\n\nThe rolled series.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"smooth\" markdown=\"1\">\n\n## function __smooth__\\(series, winlen=11, wintype=u'hanning'\\)\n\nSmooths a signal using a window with requested size.\n\nThis method is based on the convolution of a scaled window with the\nsignal. The signal is prepared by introducing reflected copies of the\nsignal (with the window size) in both ends so that transient parts are\nminimized in the begining and end part of the output signal.\n\n__Adapted from:__\n\n- <http://www.scipy.org/Cookbook/SignalSmooth>\n\n__Example:__\n\n%--\npython: |\n import numpy as np\n from matplotlib import pyplot as plt\n from datamatrix import DataMatrix, SeriesColumn, series as srs\n\n LENGTH = 5 # Number of rows\n DEPTH = 100 # Depth (or length) of SeriesColumns\n\n sinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n\n dm = DataMatrix(length=LENGTH)\n # First create five identical rows with a sinewave\n dm.y = SeriesColumn(depth=DEPTH)\n dm.y.setallrows(sinewave)\n # And add a bit of random jitter\n dm.y += np.random.random( (LENGTH, DEPTH) )\n # Smooth the traces to reduce the jitter\n dm.y2 = srs.smooth(dm.y)\n\n plt.clf()\n plt.subplot(121)\n plt.title('Original')\n plt.plot(dm.y.plottable)\n plt.subplot(122)\n plt.title('Smoothed')\n plt.plot(dm.y2.plottable)\n plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- A signal to smooth.\n\t- Type: SeriesColumn\n\n__Keywords:__\n\n- `winlen` -- The width of the smoothing window. This should be an odd integer.\n\t- Type: int\n\t- Default: 11\n- `wintype` -- The type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'. A flat window produces a moving average smoothing.\n\t- Type: str\n\t- Default: 'hanning'\n\n__Returns:__\n\nA smoothed signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"threshold\" markdown=\"1\">\n\n## function __threshold__\\(series, fnc, min\\_length=1\\)\n\nFinds samples that satisfy some threshold criterion for a given period.\n\n__Example:__\n\n%--\npython: |\n import numpy as np\n from matplotlib import pyplot as plt\n from datamatrix import DataMatrix, SeriesColumn, series as srs\n\n LENGTH = 1 # Number of rows\n DEPTH = 100 # Depth (or length) of SeriesColumns\n\n sinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n\n dm = DataMatrix(length=LENGTH)\n # First create five identical rows with a sinewave\n dm.y = SeriesColumn(depth=DEPTH)\n dm.y.setallrows(sinewave)\n # And also a bit of random jitter\n dm.y += np.random.random( (LENGTH, DEPTH) )\n # Threshold the signal by > 0 for at least 10 samples\n dm.t = srs.threshold(dm.y, fnc=lambda y: y > 0, min_length=10)\n\n plt.clf()\n # Mark the thresholded signal\n plt.fill_between(np.arange(DEPTH), dm.t[0], color='black', alpha=.25)\n plt.plot(dm.y.plottable)\n print(dm)\n plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- A signal to threshold.\n\t- Type: SeriesColumn\n- `fnc` -- A function that takes a single value and returns True if this value exceeds a threshold, and False otherwise.\n\t- Type: FunctionType\n\n__Keywords:__\n\n- `min_length` -- The minimum number of samples for which `fnc` must return True.\n\t- Type: int\n\t- Default: 1",
    "title": "datamatrix.series",
    "url": "https://pydatamatrix.eu/1.0/series",
    "path": "content/pages/series.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 2
  },
  {
    "content": "# datamatrix.series\n\n\n__Returns:__\n\nA series where 0 indicates below threshold, and 1 indicates above threshold.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"trim\" markdown=\"1\">\n\n## function __trim__\\(series, value=nan, start=False, end=True\\)\n\nTrims trailing and/ or leading values from a series. This is useful,\nfor example, to discard the end (or beginning) of a series that\nconsists exclusively of invalid data, such as `NAN` or 0 values.\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, SeriesColumn, series as srs\n \n dm = DataMatrix(length=3)\n dm.s = SeriesColumn(depth=5)\n dm.s[0] = 0, 0, 2, 0, 0\n dm.s[1] = 0, 0, 0, 3, 0\n dm.s[2] = 0, 0, 2, 3, 0\n dm.trimmed = srs.trim(dm.s, value=0, start=True, end=True)\n print(dm)\n--%\n\n__Arguments:__\n\n- `series` -- The series column to trim\n\t- Type: SeriesColumn\n\n__Keywords:__\n\n- `value` -- The value to trim\n\t- Type: int, float\n\t- Default: nan\n- `start` -- Indicates whether the start of the series should be trimmed\n\t- Type: bool\n\t- Default: False\n- `end` -- Indicates whether the end of the series should be trimmed\n\t- Type: bool\n\t- Default: True\n\n__Returns:__\n\nA trimmed copy of the series column\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"window\" markdown=\"1\">\n\n## function __window__\\(series, start=0, end=None\\)\n\nExtracts a window from a signal.\n\n*Version note:* As of 0.9.4, the preferred way to get a window from a\nseries is with a slice: `dm.s[:, start:end]`.\n\n__Example:__\n\n%--\npython: |\n import numpy as np\n from matplotlib import pyplot as plt\n from datamatrix import DataMatrix, SeriesColumn, series as srs\n\n LENGTH = 5 # Number of rows\n DEPTH = 10 # Depth (or length) of SeriesColumns\n\n sinewave = np.sin(np.linspace(0, 2*np.pi, DEPTH))\n\n dm = DataMatrix(length=LENGTH)\n # First create five identical rows with a sinewave\n dm.y = SeriesColumn(depth=DEPTH)\n dm.y.setallrows(sinewave)\n # Add a random offset to the Y values\n dm.y += np.random.random(LENGTH)\n # Look only the middle half of the signal\n dm.y2 = srs.window(dm.y, start=DEPTH//4, end=-DEPTH//4)\n\n plt.clf()\n plt.subplot(121)\n plt.title('Original')\n plt.plot(dm.y.plottable)\n plt.subplot(122)\n plt.title('Window (middle half)')\n plt.plot(dm.y2.plottable)\n plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- The signal to get a window from.\n\t- Type: SeriesColumn\n\n__Keywords:__\n\n- `start` -- The window start.\n\t- Type: int\n\t- Default: 0\n- `end` -- The window end, or None to go to the signal end.\n\t- Type: int, None\n\t- Default: None\n\n__Returns:__\n\nA window of the signal.\n\n- Type: SeriesColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"z\" markdown=\"1\">\n\n## function __z__\\(series\\)\n\nApplies a *z*-transform to the signal such that each trace has a mean\nvalue of 0 and a standard deviation of 1. That is, each trace is\n*z*-transformed individually.\n\n*Note:* If you want to *z*-transform a series column such that the mean\nof the full series is 0 with a standard deviation of 1, then use\n`operations.z()`.\n\n__Example:__\n\n%--\npython: |\n import numpy as np\n from matplotlib import pyplot as plt\n from datamatrix import DataMatrix, SeriesColumn, series as srs\n\n LENGTH = 3\n DEPTH = 200\n\n # Create one fast oscillation, and two combined fast and slow\n # oscillations\n dm = DataMatrix(length=LENGTH)\n dm.s = SeriesColumn(depth=DEPTH)\n dm.s[0] = 1 * np.sin(np.linspace(0, 4 * np.pi, DEPTH))\n dm.s[1] = 2 * np.sin(np.linspace(.4, 4.4 * np.pi, DEPTH))\n dm.s[2] = 3 * np.sin(np.linspace(.8, 4.8 * np.pi, DEPTH))\n dm.z = srs.z(dm.s)\n\n # Plot the original signal\n plt.clf()\n plt.subplot(121)\n plt.title('Original')\n plt.plot(dm.s[0])\n plt.plot(dm.s[1])\n plt.plot(dm.s[2])\n plt.subplot(122)\n # And the filtered signal!\n plt.title('Z transform')\n plt.plot(dm.z[0])\n plt.plot(dm.z[1])\n plt.plot(dm.z[2])\n plt.show()\n--%\n\n__Arguments:__\n\n- `series` -- A signal to determine the z-transform for.\n\t- Type: SeriesColumn\n\n__Returns:__\n\nThe z-transform of the signal.\n\n- Type: SeriesColumn\n\n</div>\n\n</div>\n\n",
    "title": "datamatrix.series",
    "url": "https://pydatamatrix.eu/1.0/series",
    "path": "content/pages/series.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 2,
    "total_chunks": 2
  },
  {
    "content": "# Analyzing eye-movement data\n\ntitle: Analyzing eye-movement data\n\n[TOC]\n\n## About this tutorial\n\nWe're going to analyze pupil-size data from an auditory-working-memory experiment. This data is taken from [Math\u00f4t (2018)](#references), and you can find the data and experimental materials [here](https://github.com/smathot/pupillometry_review).\n\nIn this experiment, the participant first hears a series of digits; we will refer to this period as the `sounds` trace. The number of digits (set size) varies: 3, 5, or 7. Next, there is a retention interval during which the participant keeps the digits in memory; we will refer to this period as the `retention` trace. Finally, the participant enters the response.\n\nWe will analyze pupil size during the `sounds` and `retention` traces as a function of set size. As reported by [Kahneman and Beatty (1966)](#references), and as we will also see during this tutorial, the size of the pupil increases with set size.\n\nThis tutorial makes use of the [`eyelinkparser` module](https://github.com/smathot/python-eyelinkparser), which can be installed with pip:\n\n~~~bash\npip install eyelinkparser\n~~~\n\nThe data has been collected with an EyeLink 1000 eye tracker.\n\n\n## Designing an experiment for easy analysis\n\nEyeLink data files (and data files for most other eye trackers) correspond to an event log; that is, each line corresponds to some event. These events can be gaze samples, saccade onsets, user messages, etc.\n\nFor example, a `start_trial` user message followed by four gaze samples might look like this:\n\n~~~text\nMSG\t451224 start_trial\n451224\t  517.6\t  388.9\t 1691.0\t...\n451225\t  517.5\t  389.1\t 1690.0\t...\n451226\t  517.3\t  388.9\t 1692.0\t...\n451227\t  517.1\t  388.7\t 1693.0\t...\n~~~\n\nWhen designing your experiment, it's important to send user messages in such a way that your analysis software, in this case `eyelinkparser`, knows how to interpret them. If you do, then data analysis will be easy, because you will not have to write a custom script to parse the data file from the ground up.\n\nIf you use [OpenSesame/ PyGaze](http://osdoc.cogsci.nl), most of these messages, with the exception of phase messages, will by default be sent in the below format automatically.\n\n\n### Trials\n\nThe following messages indicate the start and end of a trial. The `trialid` argument is optional.\n\n\tstart_trial [trialid]\n\tend_trial\n\n### Variables\n\nThe following message indicates a variable and a value. For example, `var response_time 645` would tell `eyelinkparser` that the variable `response_time` has the value 645 on that particular trial.\n\n\tvar [name] [value]\n\n### Phases\n\nPhases are named periods of continuous data. Defining phases during the experiment is the easiest way to segment your data into different epochs for analysis.\n\nThe following messages indicate the start and end of a phase. A phase is automatically ended when a new phase is started.\n\n\tstart_phase [name]\n\tend_phase [name]\n\nFor each phase, four columns of type `SeriesColumn` will be created with information about fixations:\n\n- `fixxlist_[phase name]` is a series of X coordinates\n- `fixylist_[phase name]` is a series of Y coordinates\n- `fixstlist_[phase name]` is a series of fixation start times\n- `fixetlist_[phase name]` is a series of fixation end times\n- `blinkstlist_[phase name]` is a series of blink start times\n- `blinketlist_[phase name]` is a series of blink end times\n\n\nAdditionally, four columns will be created with information about individual gaze samples:\n\n- `xtrace_[phase name]` is a series of X coordinates\n- `ytrace_[phase name]` is a series of Y coordinates\n- `ttrace_[phase name]` is a series of time stamps\n- `ptrace_[phase name]` is a series of pupil sizes\n\n\n## Analyzing data\n\n### Parsing\n\nWe first define a function to parse the EyeLink data; that is, we read the data files, which are in `.asc` text format, into a `DataMatrix` object.\n\nWe define a `get_data()` function that is decorated with `@fnc.memoize()` such that parsing is not redone unnecessarily (see [memoization](%link:memoization%)).\n\n```python\nfrom datamatrix import (\n  operations as ops,\n  functional as fnc,\n  series as srs\n)\nfrom eyelinkparser import parse, defaulttraceprocessor\n\n\n@fnc.memoize(persistent=True)\ndef get_data():\n\n    # The heavy lifting is done by eyelinkparser.parse()\n    dm = parse(\n        folder='data',           # Folder with .asc files\n        traceprocessor=defaulttraceprocessor(\n          blinkreconstruct=True, # Interpolate pupil size during blinks\n          downsample=10,         # Reduce sampling rate to 100 Hz,\n          mode='advanced'        # Use the new 'advanced' algorithm\n        )\n    )\n    # To save memory, we keep only a subset of relevant columns.\n    dm = dm[dm.set_size, dm.correct, dm.ptrace_sounds, dm.ptrace_retention, \n            dm.fixxlist_retention, dm.fixylist_retention]\n    return dm\n```\n\nWe now call this function to get the data as a a `DataMatrix`. If you want to clear the cache, you can call `get_data.clear()` first.\n\nLet's also print out the `DataMatrix` to get some idea of what our data structure looks like. As you can see, traces are stored as [series](https://pythontutorials.eu/numerical/time-series/), which is convenient for further analysis.\n\n```python\ndm = get_data()\nprint(dm)\n```\n\n\n### Preprocessing\n\nNext, we do some preprocessing of the pupil-size data.\n\nWe are interested in two traces, `sounds` and `retention`. The length of `sounds` varies, depending on how many digits were played back. The shorter traces are padded with `nan` values at the end. We therefore apply `srs.endlock()` to move the `nan` padding to the beginning of the trace.\n\nTo get some idea of what this means, let's plot pupil size during the `sounds` trace for the first 5 trials, both with and without applying `srs.endlock()`.\n\n```python\nfrom matplotlib import pyplot as plt\nfrom datamatrix import series as srs\n\nplt.figure()\nplt.subplot(211)\nplt.title('NANs at the end')\nfor pupil in dm.ptrace_sounds[:5]:\n    plt.plot(pupil)\nplt.subplot(212)\nplt.title('NANs at the start')\nfor pupil in srs.endlock(dm.ptrace_sounds[:5]):\n    plt.plot(pupil)\nplt.show()\n```\n\nNext, we concatenate the (end-locked) `sounds` and `retention` traces, and save the result as a series called `pupil`.\n\n```python\ndm.pupil = srs.concatenate(\n    srs.endlock(dm.ptrace_sounds),\n    dm.ptrace_retention\n)\n```\n\nWe then perform baseline correction. As a baseline, we use the first two samples of the `sounds` trace. (This trace still has the `nan` padding at the end.)\n\n```python\ndm.pupil = srs.baseline(\n    series=dm.pupil,\n    baseline=dm.ptrace_sounds,\n    bl_start=0,\n    bl_end=2\n)\n```\n\nAnd we explicitly set the depth of the `pupil` trace to 1200, which given our original 1000 Hz signal, downsampled 10 \u00d7, corresponds to 12 s.\n\n```python\ndm.pupil.depth = 1200\n```\n\n\n### Analyzing pupil size\n\nAnd now we plot the pupil traces for each of the three set sizes!\n\n```python\nimport numpy as np\n\n\ndef plot_series(x, s, color, label):\n\n    se = s.std / np.sqrt(len(s))\n    plt.fill_between(x, s.mean-se, s.mean+se, color=color, alpha=.25)\n    plt.plot(x, s.mean, color=color, label=label)\n\n\nx = np.linspace(-7, 5, 1200)\ndm3, dm5, dm7 = ops.split(dm.set_size, 3, 5, 7)\n\nplt.figure()\nplt.xlim(-7, 5)\nplt.ylim(-150, 150)\nplt.axvline(0, linestyle=':', color='black')\nplt.axhline(1, linestyle=':', color='black')\nplot_series(x, dm3.pupil, color='green', label='3 (N=%d)' % len(dm3))\nplot_series(x, dm5.pupil, color='blue', label='5 (N=%d)' % len(dm5))\nplot_series(x, dm7.pupil, color='red', label='7 (N=%d)' % len(dm7))\nplt.ylabel('Pupil size (norm)')\nplt.xlabel('Time relative to onset retention interval (s)')\nplt.legend(frameon=False, title='Memory load')\nplt.show()\n```\n\nAnd a beautiful replication of [Kahneman & Beatty (1966)](#references)!\n\n\n### Analyzing fixations\n\nNow let's look at fixations during the `retention` phase. To get an idea of how the data is structured, we print out the x, y coordinates of all fixations of the first two trials.\n\n```python\nfor i, row in zip(range(2), dm):\n    print('Trial %d' % i)\n    for x, y in zip(\n        row.fixxlist_retention,\n        row.fixylist_retention\n    ):\n        print('\\t', x, y)\n```\n\nA common way to plot fixation distributions is as a heatmap. To do this, we need to create numpy arrays from the `fixxlist_retention` and `fixylist_retention` columns. This will result in two 2D arrays, whereas `plt.hexbin()` expects two 1D arrays. So we additionally flatten the arrays.\n\nThe resulting heatmap clearly shows that fixations are clustered around the display center (512, 384), just as you would expect from an experiment in which the participant needs to maintain central fixation.\n\n```python\nimport numpy as np\n\nx = np.array(dm.fixxlist_retention)\ny = np.array(dm.fixylist_retention)\nx = x.flatten()\ny = y.flatten()\nplt.hexbin(x, y, gridsize=25)\nplt.show()\n```\n\n## References\n\n- Kahneman, D., & Beatty, J. (1966). Pupil diameter and load on memory. *Science*, 154(3756), 1583\u20131585. <https://doi.org/10.1126/science.154.3756.1583>\n- Math\u00f4t, S., (2018). Pupillometry: Psychology, Physiology, and Function. *Journal of Cognition*. 1(1), p.16. <https://doi.org/10.5334/joc.18>",
    "title": "Analyzing eye-movement data",
    "url": "https://pydatamatrix.eu/1.0/eyelinkparser",
    "path": "content/pages/eyelinkparser.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "# datamatrix.functional\n\ntitle: datamatrix.functional\n\nA set of functions and decorators for [functional programming](https://docs.python.org/3.6/howto/functional.html). This module is typically imported as `fnc` for brevity:\n\n```python\nfrom datamatrix import functional as fnc\n```\n\n\n[TOC]\n\n## What is functional programming?\n\nFunctional programming is a style of programming that is characterized by the following:\n\n- __Lack of statements__\u2014In its purest form, functional programming does not use any statements. Statements are things like assignments (e.g. `x  = 1`), `for` loops, `if` statements, etc. Instead of statements, functional programs are chains of function calls.\n- __Short functions__\u2014In the purest form of functional programming, each function is a single expression. In Python, this can be implemented through `lambda` expressions.\n- __Referential transparency__\u2014Functions are referentially transparent when they always return the same result given the same set of arguments (i.e. they are *stateless*), and when they do not alter the state of the program (i.e. they have no *side effects*).\n\n<div class=\" YAMLDoc\" id=\"\" markdown=\"1\">\n\n \n\n<div class=\"FunctionDoc YAMLDoc\" id=\"curry\" markdown=\"1\">\n\n## function __curry__\\(fnc\\)\n\nA [currying](https://en.wikipedia.org/wiki/Currying) decorator that\nturns a function with multiple arguments into a chain of partial\nfunctions, each of which takes at least a single argument. The input\nfunction may accept keywords, but the output function no longer does\n(i.e. currying turns all keywords into positional arguments).\n\n__Example:__\n\n%--\npython: |\n from datamatrix import functional as fnc\n\n @fnc.curry\n def add(a, b, c):\n\n    return a + b + c\n\n print(add(1)(2)(3)) # Curried approach with single arguments\n print(add(1, 2)(3)) # Partly curried approach\n print(add(1)(2, 3)) # Partly curried approach\n print(add(1, 2, 3)) # Original approach multiple arguments\n--%\n\n__Arguments:__\n\n- `fnc` -- A function to curry.\n\t- Type: callable\n\n__Returns:__\n\nA curried function that accepts at least the first argument, and returns a function that accepts the second argument, etc.\n\n- Type: callable\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"filter_\" markdown=\"1\">\n\n## function __filter\\___\\(fnc, obj\\)\n\nFilters rows from a datamatrix or column based on filter function\n(`fnc`).\n\nIf `obj` is a column, `fnc` should be a function that accepts a single\nvalue. If `obj` is a datamatrix, `fnc` should be a function that\naccepts a keyword `dict`, where column names are keys and cells are \nvalues. In both cases, `fnc` should return a `bool` indicating whether \nthe row or value should be included.\n\n*New in v0.8.0*: You can also directly compare a column with a function\nor `lambda` expression. However, this is different from `filter_()` in\nthat it returns a datamatrix object and not a column.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, functional as fnc\n\n dm = DataMatrix(length=5)\n dm.col = range(5)\n # Create a column with only odd values\n col_new = fnc.filter_(lambda x: x % 2, dm.col)\n print(col_new)\n # Create a new datamatrix with only odd values in col\n dm_new = fnc.filter_(lambda **d: d['col'] % 2, dm)\n print(dm_new)\n--%\n\n__Arguments:__\n\n- `fnc` -- A filter function.\n\t- Type: callable\n- `obj` -- A datamatrix or column to filter.\n\t- Type: BaseColumn, DataMatrix\n\n__Returns:__\n\nA new column or datamatrix.\n\n- Type: BaseColumn, DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"map_\" markdown=\"1\">\n\n## function __map\\___\\(fnc, obj\\)\n\nMaps a function (`fnc`) onto rows of datamatrix or cells of a column.\n\nIf `obj` is a column, the function `fnc` is mapped is mapped onto each\ncell of the column, and a new column is returned. In this case,\n`fnc` should be a function that accepts and returns a single value.\n\nIf `obj` is a datamatrix, the function `fnc` is mapped onto each row,\nand a new datamatrix is returned. In this case, `fnc` should be a\nfunction that accepts a keyword `dict`, where column names are keys and\ncells are values. The return value should be another `dict`, again with\ncolumn names as keys, and cells as values. Columns that are not part of\nthe returned `dict` are left unchanged.\n\n*New in v0.8.0*: In Python 3.5 and later, you can also map a function\nonto a column using the `@` operator:\n`dm.new = dm.old @ (lambda i: i*2)`\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, functional as fnc\n\n dm = DataMatrix(length=3)\n dm.old = 0, 1, 2\n # Map a 2x function onto dm.old to create dm.new\n dm.new = fnc.map_(lambda i: i*2, dm.old)\n print(dm)\n # Map a 2x function onto the entire dm to create dm_new, using a fancy\n # dict comprehension wrapped inside a lambda function.\n dm_new = fnc.map_(\n    lambda **d: {col : 2*val for col, val in d.items()},\n    dm\n )\n print(dm_new)\n--%\n\n__Arguments:__\n\n- `fnc` -- A function to map onto each row or each cell.\n\t- Type: callable\n- `obj` -- A datamatrix or column to map `fnc` onto.\n\t- Type: BaseColumn, DataMatrix\n\n__Returns:__\n\nA new column or datamatrix.\n\n- Type: BaseColumn, DataMatrix\n\n</div>\n\n<div class=\"ClassDoc YAMLDoc\" id=\"memoize\" markdown=\"1\">\n\n## class __memoize__\n\n*Requires json_tricks*\n\nA memoization decorator that stores the result of a function call, and\nreturns the stored value when the function is called again with the\nsame arguments. That is, memoization is a specific kind of caching that\nimproves performance for expensive function calls.\n\nThis decorator only works for return values that can be pickled, and\narguments that can be serialized to `json`.\n\nThe memoized function becomes a callable object. To clear the\nmemoization cache, call the `.clear()` function on the memoized\nfunction. The total size of all cached return values is available as \nthe `.cache_size` property.\n\nFor a more detailed description, see:\n\n- %link:memoization%\n\n*Changed in v0.8.0*: You can no longer pass the `memoclear` keyword to\nthe memoized function. Use the `.clear()` function instead.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import functional as fnc\n\n @fnc.memoize\n def add(a, b):\n\n    print('add(%d, %d)' % (a, b))\n    return a + b\n\n three = add(1, 2)  # Storing result in memory\n three = add(1, 2)  # Re-using previous result\n add.clear()  # Clear cache, but only for the next call\n three = add(1, 2)  # Calculate again\n\n @fnc.memoize(persistent=True, key='persistent-add')\n def persistent_add(a, b):\n\n    print('persistent_add(%d, %d)' % (a, b))\n    return a + b\n\n three = persistent_add(1, 2)  # Writing result to disk\n three = persistent_add(1, 2)  # Re-using previous result\n--%\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"profile\" markdown=\"1\">\n\n## function __profile__\\(\\*args, \\*\\*kwds\\)\n\nA context manager (`with`) for easy profiling, using cProfile. The\nresults of the profile are written to the file specified in the `path`\nkeyword (default=`u'profile'`), and the sorting order, as accepted by\n`pstats.Stats.sort_stats()`, is specified in the the `sortby` keyword\n(default=`u'cumulative'`).\n\n__Example:__\n\n%--\npython: |\n from datamatrix import functional as fnc\n\n with fnc.profile(path=u'profile.txt', sortby=u'cumulative'):\n     dm = DataMatrix(length=1000)\n     dm.col = range(1000)\n     dm.is_even = dm.col @ (lambda x: not x % 2)\n--%\n\n__Argument list:__\n\n- `*args`: No description.\n\n__Keyword dict:__\n\n- `**kwds`: No description.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"setcol\" markdown=\"1\">\n\n## function __setcol__\\(dm, name, value\\)\n\nReturns a new DataMatrix to which a column has been added or in which\na column has been modified.\n\nThe main difference with regular assignment (`dm.col = 'x'`) is that\n`setcol()` does not modify the original DataMatrix, and can be used in\n`lambda` expressions.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, functional as fnc\n\n dm1 = DataMatrix(length=5)\n dm2 = fnc.setcol(dm1, 'y', range(5))\n print(dm2)\n--%\n\n__Arguments:__\n\n- `dm` -- A DataMatrix.\n\t- Type: DataMatrix\n- `name` -- A column name.\n\t- Type: str\n- `value` -- The value to be assigned to the column. This can be any value this is valid for a regular column assignment.\n\n__Returns:__\n\nA new DataMatrix.\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"stack_multiprocess\" markdown=\"1\">\n\n## function __stack\\_multiprocess__\\(fnc, args, processes=None\\)\n\nFacilitates multiprocessing for functions that return `DataMatrix`\nobjects.\n\nSpecifically, `stack_multiprocess()`, calls `fnc()` in separate\nprocesses, each time passing a different argument. Arguments are\nspecified in `args`, which should be a list (or other iterable) of\narguments that are passed to `fnc()` for each call separately. In other\nwords, as many processes are launched as there are elements in `args`.\n`fnc()` should be a function that accepts a single argument and returns\na `DataMatrix` object. The resulting `DataMatrix` objects are stacked\ntogether (similar to `ops.stack()`) and returned as a single \n`DataMatrix`.\n\nSee also:\n\n- <https://docs.python.org/3/library/multiprocessing.html>\n- %link:operations%#function-stack\n\n*Version note:* New in 1.0.0.\n\n*Version note:* As of 1.0.4, if one of the processes crashes, and error\nis shown with the Exception, but the main process doesn't crash.\n\n__Example:__\n\n```python\nfrom datamatrix import DataMatrix, functional as fnc\n\ndef get_dm(i):\n    dm = DataMatrix(length=1)\n    dm.s = i\n    return dm\n\n# This will launch five separate processes and return a single dm\ndm = fnc.stack_multiprocess(get_dm, [1, 2, 3, 4, 5])\n```\n\narguments:\n    fnc:\n        desc: A function to call. This function should accept a single\n              argument and return a single `DataMatrix`.\n        type: callable\n    args:\n        desc: A `list` of arguments that are passes separately to\n              `fnc()`.\n\nkeywords:\n    processes:\n        desc: The number of processes that are launched simultaneously\n              or `None` to launch one process for each core on the\n              system.\n        type: [None, int]\n\nreturns:\n    type: `DataMatrix`\n\n__Arguments:__\n\n- `fnc` -- No description\n- `args` -- No description\n\n__Keywords:__\n\n- `processes` -- No description\n\t- Default: None\n\n</div>\n\n</div>\n\n",
    "title": "datamatrix.functional",
    "url": "https://pydatamatrix.eu/1.0/functional",
    "path": "content/pages/functional.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "# datamatrix.io\n\ntitle: datamatrix.io\n\nA set of functions for reading and writing `DataMatrix` objects from and to file.\n\n```python\nfrom datamatrix import io\n```\n\n\n[TOC]\n\n<div class=\" YAMLDoc\" id=\"\" markdown=\"1\">\n\n \n\n<div class=\"FunctionDoc YAMLDoc\" id=\"readbin\" markdown=\"1\">\n\n## function __readbin__\\(path\\)\n\nReads a DataMatrix from a binary file. This format allows you to read\nand write DataMatrix objects with unloaded columns, i.e. columns that\nare too large to fit in memory.\n\n__Example:__\n\n~~~.python\ndm = io.readbin('data.dm')\n~~~\n\n*Version note:* New in 1.0.0\n\n__Arguments:__\n\n- `path` -- The path to the binary file.\n\n__Returns:__\n\nA DataMatrix.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"readpickle\" markdown=\"1\">\n\n## function __readpickle__\\(path\\)\n\nReads a DataMatrix from a pickle file.\n\n__Example:__\n\n~~~.python\ndm = io.readpickle('data.pkl')\n~~~\n\n__Arguments:__\n\n- `path` -- The path to the pickle file.\n\n__Returns:__\n\nA DataMatrix.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"readtxt\" markdown=\"1\">\n\n## function __readtxt__\\(path, delimiter=u',', quotechar=u'\"', default\\_col\\_type=<class 'datamatrix\\.\\_datamatrix\\.\\_mixedcolumn\\.MixedColumn'>\\)\n\nReads a DataMatrix from a csv file.\n\n__Example:__\n\n~~~ .python\ndm = io.readtxt('data.csv')\n~~~\n\n*Version note:* As of 0.10.7, byte-order marks (BOMs) are silently\nstripped from column names.\n\n__Arguments:__\n\n- `path` -- The path to the pickle file.\n\n__Keywords:__\n\n- `delimiter` -- The delimiter characer.\n\t- Default: ','\n- `quotechar` -- The quote character.\n\t- Default: '\"'\n- `default_col_type` -- The default column type.\n\t- Default: <class 'datamatrix._datamatrix._mixedcolumn.MixedColumn'>\n\n__Returns:__\n\nA DataMatrix.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"readxlsx\" markdown=\"1\">\n\n## function __readxlsx__\\(path, default\\_col\\_type=<class 'datamatrix\\.\\_datamatrix\\.\\_mixedcolumn\\.MixedColumn'>, sheet=None\\)\n\nReads a DataMatrix from an Excel 2010 xlsx file.\n\n__Example:__\n\n~~~.python\ndm = io.readxlsx('data.xlsx')\n~~~\n\n__Arguments:__\n\n- `path` -- The path to the xlsx file.\n\n__Keywords:__\n\n- `default_col_type` -- The default column type.\n\t- Default: <class 'datamatrix._datamatrix._mixedcolumn.MixedColumn'>\n- `sheet` -- The name of a sheet, or None to open the active sheet. The activate sheet is not necessarily the first sheet. *(New in 0.7.0)*\n\t- Default: None\n\n__Returns:__\n\nA DataMatrix.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"writebin\" markdown=\"1\">\n\n## function __writebin__\\(dm, path\\)\n\nReads a DataMatrix to a binary file. This format allows you to read\nand write DataMatrix objects with unloaded columns, i.e. columns that\nare too large to fit in memory.\n\n__Example:__\n\n~~~ .python\nio.writebin(dm, 'data.dm')\n~~~\n\n*Version note:* New in 1.0.0\n\n__Arguments:__\n\n- `dm` -- The DataMatrix to write.\n- `path` -- The path to the binary file.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"writepickle\" markdown=\"1\">\n\n## function __writepickle__\\(dm, path, protocol=-1\\)\n\nWrites a DataMatrix to a pickle file.\n\n__Example:__\n\n~~~ .python\nio.writepickle(dm, 'data.pkl')\n~~~\n\n__Arguments:__\n\n- `dm` -- The DataMatrix to write.\n- `path` -- The path to the pickle file.\n\n__Keywords:__\n\n- `protocol` -- The pickle protocol.\n\t- Default: -1\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"writetxt\" markdown=\"1\">\n\n## function __writetxt__\\(dm, path, delimiter=u',', quotechar=u'\"'\\)\n\nWrites a DataMatrix to a csv file.\n\n__Example:__\n\n~~~ .python\nio.writetxt(dm, 'data.csv')\n~~~\n\n__Arguments:__\n\n- `dm` -- The DataMatrix to write.\n- `path` -- The path to the pickle file.\n\n__Keywords:__\n\n- `delimiter` -- The delimiter characer.\n\t- Default: ','\n- `quotechar` -- The quote character.\n\t- Default: '\"'\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"writexlsx\" markdown=\"1\">\n\n## function __writexlsx__\\(dm, path\\)\n\nWrites a DataMatrix to an Excel 2010 xlsx file. The first sheet will\ncontain a regular table with all non-series columns. SeriesColumns are\nsaved as individual sheets.\n\n__Example:__\n\n~~~ .python\nio.writexlsx(dm, 'data.xlsx')\n~~~\n\n__Arguments:__\n\n- `dm` -- The DataMatrix to write.\n- `path` -- The path to the xlsx file.\n\n</div>\n\n</div>\n\n",
    "title": "datamatrix.io",
    "url": "https://pydatamatrix.eu/1.0/io",
    "path": "content/pages/io.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "# Memoization (caching)\n\ntitle: Memoization (caching)\n\n[TOC]\n\n\n## What is memoization?\n\n[Memoization](https://en.wikipedia.org/wiki/Memoization) is a way to optimize code by storing the return values of functions called with a specific set of arguments. Memoization is a specific type of caching.\n\n\n## When (not) to memoize?\n\nMemoization is only valid for functions that are *referentially transparent*: functions that always return the same result for the same set of arguments, and that do not affect the state of the program.\n\nTherefore, you should *not* memoize a function that returns random numbers, because it will end up returning the same set of random numbers over and over again. And you should also *not* memoize a function that depends on the state of the program, for example because it relies on the command-line arguments that were passed to a script.\n\nBut you *could* memoize a function that performs some time consuming operation that is always done in exactly the same way, such as a function that performs time-consuming operations on a large dataset.\n\n\n## Examples\n\n### Basic memoization\n\nMemoization is done with the `memoize` decorator, which is part of [`datamatrix.functional`](%link:functional%). Let's take a time-consuming function that determines the highest prime number below a certain value, and measure the performance improvement that memoization gives us when we call the function twice with same argument.\n\n%--\npython: |\n import time\n from itertools import dropwhile\n from datamatrix import functional as fnc\n\n\n @fnc.memoize\n def prime_below(x):\n\n \t\"\"\"Returns the highest prime that is lower than X.\"\"\"\n\n \tprint('Calculating the highest prime number below %d' % x)\n \treturn next(\n \t\tdropwhile(\n \t\t\tlambda x: any(not x % i for i in range(x-1, 2, -1)),\n \t\t\trange(x-1, 0, -1)\n \t\t\t)\n \t\t)\n\n\n t0 = time.time()\n prime_below(10000)\n t1 = time.time()\n prime_below(10000)\n t2 = time.time()\n\n print('Fresh: %.2f ms' % (1000*(t1-t0)))\n print('Memoized: %.2f ms' % (1000*(t2-t1)))\n--%\n\n\n### Chaining memoized functions and lazy evaluation\n\nWhen you call a function, Python automatically evaluates the function arguments. This happens even if a function has been memoized. In some cases, this is undesirable because evaluating the arguments may be time-consuming in itself, for example because one of the arguments is a call to another time-consuming function.\n\nIdeally, evaluation of the arguments occurs only when the memoized function actually needs to be executed. To approximate this behavior in Python, the `memoize` decorator accepts the `lazy` keyword. When `lazy=True` is specified, all callable objects that are passed to the memoized function are evaluated automatically, but *only* when the memoized function is actually executed.\n\n%--\npython: |\n @fnc.memoize(lazy=True)\n def prime_below(x):\n\n \tprint('Calculating the highest prime number below %d' % x)\n \treturn next(\n \t\tdropwhile(\n \t\t\tlambda x: any(not x % i for i in range(x-1, 2, -1)),\n \t\t\trange(x-1, 0, -1)\n \t\t\t)\n \t\t)\n\n\n def thousand():\n\n \tprint('Returning a thousand!')\n \treturn 1000\n\n\n print(prime_below(thousand))\n print(prime_below(thousand))\n--%\n\n\nA slightly more complicated situation arises when you want to pass arguments to a function that is itself passed as argument, without evaluating the function. To accomplish this, you can first bind the argument to the function using `functools.partial` and then pass the resulting partial function as an argument. Like so:\n\n%--\npython: |\n from functools import partial\n\n print(prime_below(partial(prime_below, 1000)))\n print(prime_below(partial(prime_below, 1000)))\n--%\n\nYou can also implement this behavior with the `>>` operator, in which the resulting of one function call is fed into the next function call, etc. The result is a `chain` object that needs to be explicitly called. The `>>` only works\nwith lazy memoization.\n\n%--\npython: |\n chain = 1000 >> prime_below >> prime_below\n print(chain())\n print(chain())\n--%\n\n\n### Persistent memoization, memoization keys, and cache clearing\n\nIf you pass `persistent=True` to the `memoize` decorator, the cache will be written to disk, by default to a subfolder `.memoize` of the current working directory. The filename will correspond to the memoization key, which by default is derived from the function name and the arguments.\n\nIf you want to change the cache folder, you can either pass a `folder` keyword to the `memoize` decorator, or change the `memoize.folder` class property before applying the `memoize` decorator to any functions.\n\nYou can also specify a custom memoization key through the `key` keyword. If you specify a custom key, `memoize` will no longer distinguish between different arguments (and thus no longer be real `memoization`).\n\nTo re-execute a memoized function, you can clear the memoization cache by calling the `.clear()` method on the memoized function, as shown below. This will clear the cache only for the next function call.\n\n\n%--\npython: |\n @fnc.memoize(persistent=True, key='custom-key')\n def prime_below(x):\n\n \tprint('Calculating the highest prime number below %d' % x)\n \treturn next(\n \t\tdropwhile(\n \t\t\tlambda x: any(not x % i for i in range(x-1, 2, -1)),\n \t\t\trange(x-1, 0, -1)\n \t\t\t)\n \t\t)\n\n\n print(prime_below(1000))\n print(prime_below(1000))\n prime_below.clear() # Clear the cache\n print(prime_below(1000))\n--%\n\n\n## Limitations\n\nMemoization only works for functions with:\n\n- Arguments and keywords that:\n\t- Can be serialized by `json_tricks`, which includes simple data types, DataMatrix objects, and numpy array; or\n\t- Are callable, which includes regular functions, `lambda` expressions, `partial` objects, and `memoize` objects.\n- Return values that can be pickled.",
    "title": "Memoization (caching)",
    "url": "https://pydatamatrix.eu/1.0/memoization",
    "path": "content/pages/memoization.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "# Install\n\ntitle: Install\n\n\n## Dependencies\n\n`DataMatrix` requires only the Python standard library. That is, you can use it without installing any additional Python packages (although the pip and conda packages install some of the optional dependencies by default). Python 3.7 and higher are supported.\n\nThe following packages are required for extra functionality:\n\n- `numpy` and `scipy` for using the `FloatColumn`, `IntColumn`, `SeriesColumn`, `MultiDimensionalColumn` objects\n- `pandas` for conversion to and from `pandas.DataFrame`\n- `mne` for conversion to and from `mne.Epochs` and `mne.TFR`\n- `fastnumbers` for improved performance\n- `prettytable` for creating a text representation of a DataMatrix (e.g. to print it out)\n- `openpyxl` for reading and writing `.xlsx` files\n- `json_tricks` for hashing, serialization to and from `json`, and memoization (caching)\n- `tomlkit` for reading configuration from `pyproject.toml`\n- `psutil` for dynamic loading of large data\n\n\n## Installation\n\n### PyPi (pip install)\n\n~~~bash\npip install datamatrix\n~~~\n\n\n### Anaconda\n\n~~~bash\nconda install -c conda-forge datamatrix\n~~~\n\n\n### Ubuntu\n\n~~~bash\nsudo add-apt-repository ppa:smathot/cogscinl  # for stable releases\nsudo add-apt-repository ppa:smathot/rapunzel  # for development releases\nsudo apt-get update\nsudo apt install python3-datamatrix\n~~~\n\n\n## Source code\n\n- <https://github.com/open-cogsci/python-datamatrix>",
    "title": "Install",
    "url": "https://pydatamatrix.eu/1.0/install",
    "path": "content/pages/install.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "# datamatrix.convert\n\ntitle: datamatrix.convert\n\nA set of functions to convert `DataMatrix` and column objects to and from other data structures, notable `pandas.DataFrame`, `mne.Epochs()`, and JSON strings. This module is typically imported as `cnv` for brevity:\n\n```python\nfrom datamatrix import convert as cnv\n```\n\n[TOC]\n\n<div class=\" YAMLDoc\" id=\"\" markdown=\"1\">\n\n \n\n<div class=\"FunctionDoc YAMLDoc\" id=\"from_json\" markdown=\"1\">\n\n## function __from\\_json__\\(s\\)\n\n*Requires json_tricks*\n\nCreates a DataMatrix from a `json` string.\n\n__Arguments:__\n\n- `s` -- A json string.\n\t- Type: str\n\n__Returns:__\n\nA DataMatrix.\n\n- Type: DataMatrix.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"from_mne_epochs\" markdown=\"1\">\n\n## function __from\\_mne\\_epochs__\\(epochs, ch\\_avg=False\\)\n\n[Python MNE](https://mne.tools/) is a library for analysis of\nneurophysiological data.\n\nThis function converts an `mne.Epochs()` object to a multidimensional\ncolumn. The column's metadata is set to the `epochs.info` property. The\nlength of the datamatrix should match the length of the metadata that\nwas passed to `mne.Epochs()`. Rejected epochs result in `nan` values\nin the corresponding rows of the column.\n\nIf channels are averaged, the shape of the resulting column is\ntwo-dimensional, where the first dimension is the number of rows and\nthe second dimension is sample time.\n\nIf channels not are averaged, the shape of the resulting column is\nthree-dimensional, where the first dimension is the number of rows,\nthe second dimension is the channel (which can be referenced by name),\nand the third dimension is sample time.\n\n*Version note:* New in 1.0.0\n\n%--\npython: |\n    import numpy as np\n    import pickle\n    import mne\n    from matplotlib import pyplot as plt\n    from datamatrix import operations as ops, convert as cnv\n    \n    # First read a simple dataset that contains data from three occipital EEG\n    # channels (O1, O2, Oz). `events` is an mne style array with event codes\n    # and timestamps. `dm` is a datamatrix with trial information.\n    with open('data/eeg-data.pkl', 'rb') as fd:\n        raw, events, dm = pickle.loads(fd.read())\n    # Create an Epochs object and convert it to a multidimensional column (dm.erp)\n    epochs = mne.Epochs(raw, events, tmin=-.05, tmax=1.5,\n                        metadata=cnv.to_pandas(dm))\n    dm.erp = cnv.from_mne_epochs(epochs)\n    # The last dimension corresponds to time, and the `index_names` property\n    # contains the timestamps in seconds.\n    channels = dm.erp.index_names[0]\n    timestamps = dm.erp.index_names[1]\n    # Split the data by stimulus intensity, and plot the mean signal over time\n    plt.title(f'channels = {channels}')\n    for intensity, idm in ops.split(dm.intensity):\n        # Average over trials and channels, but not time\n        plt.plot(timestamps, idm.erp[..., ...], label=str(intensity))\n    plt.legend(title='Stimulus intensity')\n    plt.xlabel('Time (s)')\n    plt.ylabel('Voltage')\n    plt.show()\n--%\n\n__Arguments:__\n\n- `epochs` -- No description\n\t- Type: mne.Epochs\n\n__Keywords:__\n\n- `ch_avg` -- Determines whether the epochs should be averaged across channels or not.\n\t- Type: bool\n\t- Default: False\n\n__Returns:__\n\nNo description\n\n- Type: MultiDimensionalColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"from_mne_tfr\" markdown=\"1\">\n\n## function __from\\_mne\\_tfr__\\(tfr, ch\\_avg=False, freq\\_avg=False\\)\n\n[Python MNE](https://mne.tools/) is a library for analysis of\nneurophysiological data.\n\nThis function converts an `mne.EpochsTFR()` object to a \nmultidimensional column. The column's metadata is set to the\n`tfr.info` property. The length of the datamatrix should match the \nlength of the metadata that was passed to `mne.Epochs()`. Rejected \nepochs result in `nan` values in the corresponding rows of the column.\n\nIf both channels and frequencies are averaged, the shape of the\nresulting column is two-dimensional, where the first dimension is the \nnumber of rows and the second dimension is sample time.\n\nIf only channels are averaged, the shape of the resulting column is\nthree-dimensional, where the first dimension is the number of rows, the\nsecond dimension is frequency, and the third dimension is sample time.\n\nIf only frequencies are averaged, the shape of the resulting column is\nthree-dimensional, where the first dimension is the number of rows, the\nsecond dimension is channel (which can be referenced by name), and the\nthird dimension is sample time.\n\nIf nothing is averaged, the shape of the resulting column is\nfour-dimensional, where the first dimension is the number of rows,\nthe second dimension is the channel (which can be referenced by name),\nthe third dimension is frequency, and the fourth dimension is sample\ntime.\n\n*Version note:* New in 1.0.0\n\n%--\npython: |\n    import numpy as np\n    import pickle\n    import mne\n    from mne.time_frequency import tfr_morlet\n    from matplotlib import pyplot as plt\n    from datamatrix import operations as ops, convert as cnv\n    \n    # First read a simple dataset that contains data from three occipital EEG\n    # channels (O1, O2, Oz). `events` is an mne style array with event codes\n    # and timestamps. `dm` is a datamatrix with trial information.\n    with open('data/eeg-data.pkl', 'rb') as fd:\n        raw, events, dm = pickle.loads(fd.read())\n    # Create an Epochs object. From there, create a TFR object. From there, create\n    # a multidimensional column.\n    epochs = mne.Epochs(raw, events, tmin=-.5, tmax=2, metadata=cnv.to_pandas(dm))\n    tfr = tfr_morlet(epochs, freqs=np.arange(4, 30, 1), n_cycles=2,\n                     return_itc=False, average=False)\n    tfr.crop(0, 1.5)\n    dm.tfr = cnv.from_mne_tfr(tfr)\n    # Plot the \n    channels = dm.tfr.index_names[0]\n    plt.title(f'channels = {channels}')\n    # Average over trials and channels, but not frequency or time.\n    plt.imshow(dm.tfr[..., ...], aspect='auto')\n    plt.xticks(dm.tfr.index_values[2][::100], dm.tfr.index_names[2][::100])\n    plt.yticks(dm.tfr.index_values[1][::4], dm.tfr.index_names[1][::4])\n    plt.xlabel('Time (s)')\n    plt.ylabel('Frequency (Hz)')\n    plt.show()\n--%\n\n__Arguments:__\n\n- `tfr` -- No description\n\t- Type: mne.EpochsTFR\n\n__Keywords:__\n\n- `ch_avg` -- Determines whether the data should be averaged across channels or not.\n\t- Type: bool\n\t- Default: False\n- `freq_avg` -- Determines whether the data should be averaged across frequencies or not.\n\t- Type: bool\n\t- Default: False\n\n__Returns:__\n\nNo description\n\n- Type: MultiDimensionalColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"from_pandas\" markdown=\"1\">\n\n## function __from\\_pandas__\\(df\\)\n\nConverts a pandas DataFrame to a DataMatrix.\n\n__Example:__\n\n%--\npython: |\n import pandas as pd\n from datamatrix import convert\n\n df = pd.DataFrame( {'col' : [1,2,3] } )\n dm = convert.from_pandas(df)\n print(dm)\n--%\n\n__Arguments:__\n\n- `df` -- No description\n\t- Type: DataFrame\n\n__Returns:__\n\nNo description\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"to_json\" markdown=\"1\">\n\n## function __to\\_json__\\(dm\\)\n\n*Requires json_tricks*\n\nCreates (serializes) a `json` string from a DataMatrix.\n\n__Arguments:__\n\n- `dm` -- A DataMatrix to serialize.\n\t- Type: DataMatrix\n\n__Returns:__\n\nA json string.\n\n- Type: str\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"to_pandas\" markdown=\"1\">\n\n## function __to\\_pandas__\\(obj\\)\n\nConverts a DataMatrix to a pandas DataFrame, or a column to a Series.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, convert\n\n dm = DataMatrix(length=3)\n dm.col = 1, 2, 3\n df = convert.to_pandas(dm)\n print(df)\n--%\n\n__Arguments:__\n\n- `obj` -- No description\n\t- Type: DataMatrix, BaseColumn\n\n__Returns:__\n\nNo description\n\n- Type: DataFrame, Series\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"wrap_pandas\" markdown=\"1\">\n\n## function __wrap\\_pandas__\\(fnc\\)\n\nA decorator for pandas functions. It converts a DataMatrix to a DataFrame, passes it to a function, and then converts the returned DataFrame back to a DataMatrix.\n\n__Example:__\n\n~~~ .python\nimport pandas as pd\nfrom datamatrix import convert as cnv\n\npivot_table = cnv.wrap_pandas(pd.pivot_table)\n~~~\n\n__Arguments:__\n\n- `fnc` -- A function that takes a DataFrame as first argument and returns a DataFrame as sole return argument.\n\t- Type: callable\n\n__Returns:__\n\nA function takes a DataMatrix as first argument and returns a DataMatrix as sole return argument.\n\n</div>\n\n</div>\n\n",
    "title": "datamatrix.convert",
    "url": "https://pydatamatrix.eu/1.0/convert",
    "path": "content/pages/convert.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "# Basic use\n\ntitle: Basic use\n\n\n[TOC]\n\n\n## Ultra-short cheat sheet\n\n~~~python\nfrom datamatrix import DataMatrix, io\n# Read a DataMatrix from file\ndm = io.readtxt('data.csv')\n# Create a new DataMatrix\ndm = DataMatrix(length=5)\n# The first two rows\nprint(dm[:2])\n# Create a new column and initialize it with the Fibonacci series\ndm.fibonacci = 0, 1, 1, 2, 3\n# You can also specify column names as if they are dict keys\ndm['fibonacci'] = 0, 1, 1, 2, 3\n# Remove 0 and 3 with a simple selection\ndm = (dm.fibonacci > 0) & (dm.fibonacci < 3)\n# Get a list of indices that match certain criteria\nprint(dm[(dm.fibonacci > 0) & (dm.fibonacci < 3)])\n# Select 1, 1, and 2 by matching any of the values in a set\ndm = dm.fibonacci == {1, 2}\n# Select all odd numbers with a lambda expression\ndm = dm.fibonacci == (lambda x: x % 2)\n# Change all 1s to -1\ndm.fibonacci[dm.fibonacci == 1] = -1\n# The first two cells from the fibonacci column\nprint(dm.fibonacci[:2])\n# Column mean\nprint(dm.fibonacci[...])\n# Multiply all fibonacci cells by 2\ndm.fibonacci_times_two = dm.fibonacci * 2\n# Loop through all rows\nfor row in dm:\n    print(row.fibonacci) # get the fibonacci cell from the row\n# Loop through all columns\nfor colname, col in dm.columns:\n    for cell in col: # Loop through all cells in the column\n        print(cell) # do something with the cell\n# Or just see which columns exist\nprint(dm.column_names)\n~~~\n\n__Important note:__ Because of a limitation (or feature, if you will) of the Python language, the behavior of `and`, `or`, and chained (`x < y < z`) comparisons cannot be modified. These therefore do not work with `DataMatrix` objects as you would expect them to:\n\n~~~python\n# INCORRECT: The following does *not* work as expected\ndm = dm.fibonacci > 0 and dm.fibonacci < 3\n# INCORRECT: The following does *not* work as expected\ndm = 0 < dm.fibonacci < 3\n# CORRECT: Use the '&' operator\ndm = (dm.fibonacci > 0) & (dm.fibonacci < 3)\n~~~\n\nSlightly longer cheat sheet:\n\n\n## Creating a DataMatrix\n\nCreate a new `DataMatrix` object with a length (number of rows) of 2, and add a column (named `col`). By default, the column is of the `MixedColumn` type, which can store numeric, string, and `None` data.\n\n```python\nimport sys\nfrom datamatrix import DataMatrix, __version__\ndm = DataMatrix(length=2)\ndm.col = '\u263a'\nprint('DataMatrix v{} on Python {}\\n'.format(__version__, sys.version))\nprint(dm)\n```\n\nYou can change the length of the `DataMatrix` later on. If you reduce the length, data will be lost. If you increase the length, empty cells (by default containing empty strings) will be added.\n\n```python\ndm.length = 3\n```\n\n## Reading and writing files\n\nYou can read and write files with functions from the `datamatrix.io` module. The main supported file types are `csv` and `xlsx`.\n\n```python\nfrom datamatrix import io\n\ndm = DataMatrix(length=3)\ndm.col = 1, 2, 3\n# Write to disk\nio.writetxt(dm, 'my_datamatrix.csv')\nio.writexlsx(dm, 'my_datamatrix.xlsx')\n# And read it back from disk!\ndm = io.readtxt('my_datamatrix.csv')\ndm = io.readxlsx('my_datamatrix.xlsx')\n```\n\nMultidimensional columns cannot be saved to `csv` or `xlsx` format but instead need to be saved to a custom binary format.\n\n```\nfrom datamatrix import MultiDimensionalColumn\ndm.mdim_col = MultiDimensionalColumn(shape=2)\n# Write to disk\nio.writebin(dm, 'my_datamatrix.dm')\n# And read it back from disk!\ndm = io.readbin('my_datamatrix.dm')\n```\n\n\n## Stacking (vertically concatenating) DataMatrix objects\n\nYou can stack two `DataMatrix` objects using the `<<` operator. Matching columns will be combined. (Note that row 2 is empty. This is because we have increased the length of `dm` in the previous step, causing an empty row to be added.)\n\n\n```python\ndm2 = DataMatrix(length=2)\ndm2.col = '\u263a'\ndm2.col2 = 10, 20\ndm3 = dm << dm2\nprint(dm3)\n```\n\nPro-tip: To stack three or more `DataMatrix` objects, using [the `stack()` function from the `operations` module](%url:operations) is faster than iteratively using the `<<` operator.\n\n```python\nfrom datamatrix import operations as ops\ndm4 = ops.stack(dm, dm2, dm3)\n```\n\n## Working with columns\n\n### Referring to columns\n\nYou can refer to columns in two ways: as keys in a `dict` or as properties. The two notations are identical for most purposes. The main reason to use a `dict` style is when the name of the column is itself variable. Otherwise, the property style is recommended for clarity.\n\n```python\ndm['col']  # dict style\ndm.col     # property style\n```\n\n### Creating columns\n\nBy assigning a value to a non-existing colum, a new column is created and initialized to this value.\n\n```python\ndm.col = 'Another value'\nprint(dm)\n```\n\n\n### Renaming columns\n\n\n```python\ndm.rename('col', 'col2')\nprint(dm)\n```\n\n### Deleting columns\n\nYou can delete a column using the `del` keyword:\n\n\n```python\ndm.col = 'x'\ndel dm.col2\nprint(dm)\n```\n\n### Column types\n\nThere are five column types:\n\n- `MixedColumn` is the default column type. This can contain numbers (`int` and `float`), strings (`str`), and `None` values. This column type is flexible but not very fast because it is (mostly) implemented in pure Python, rather than using `numpy`, which is the basis for the other columns. The default value for empty cells is an empty string.\n- `FloatColumn` contains `float` numbers. The default value for empty cells is `NAN`.\n- `IntColumn` contains `int` numbers. (This does not include `INF`, and `NAN`, which are of type `float` in Python.) The default value for empty cells is 0.\n- `MultiDimensionalColumn` contains higher-dimensional `float` arrays. This allows you to mix higher-dimensional data, such as time series or images, with regular one-dimensional data. The default value for empty cells is `NAN`.\n- `SeriesColumn` is identical to a two-dimensional `MultiDimensionalColumn`.\n\nWhen you create a `DataMatrix`, you can indicate a default column type.\n\n```python\n# Create IntColumns by default\ndm = DataMatrix(length=2, default_col_type=int)\ndm.i = 1, 2  # This is an IntColumn\n```\n\nYou can also explicitly indicate the column type when creating a new column. \n\n```python\ndm.f = float  # This creates an empty (`NAN`-filled) FloatColumn\ndm.i = int    # This creates an empty (0-filled) IntColumn\n```\n\nTo create a `MultiDimensionalColumn` you need to import the column type and specify a shape:\n\n```python\nfrom datamatrix import MultiDimensionalColumn\ndm.mdim_col = MultiDimensionalColumn(shape=(2, 3))\nprint(dm)\n```\n\nYou can also specify named dimensions. For example, `('x', 'y')` creates a dimension of size 2 where index 0 can be referred to as 'x' and index 1 can be referred to as 'y':\n\n```python\ndm.mdim_col = MultiDimensionalColumn(shape=(('x', 'y'), 3))\n```\n\n\n### Column properties\n\nBasic numerical properties, such as the mean, can be accessed directly. For this purpose, only numerical, non-`NAN` values are taken into account.\n\n\n```python\ndm = DataMatrix(length=3)\ndm.col = 1, 2, 'not a number'\n# Numeric descriptives\nprint('mean: %s' % dm.col.mean)  #  or dm.col[...]\nprint('median: %s' % dm.col.median)\nprint('standard deviation: %s' % dm.col.std)\nprint('sum: %s' % dm.col.sum)\nprint('min: %s' % dm.col.min)\nprint('max: %s' % dm.col.max)\n# Other properties\nprint('unique values: %s' % dm.col.unique)\nprint('number of unique values: %s' % dm.col.count)\nprint('column name: %s' % dm.col.name)\n```\n\nThe `shape` property indicates the number and sizes of the dimensions of the column. For regular columns, the shape is a tuple containing only the length of the datamatrix (the number of rows). For multidimensional columns, the shape is a tuple containing the length of the datamatrix and the shape of cells as specified through the `shape` keyword.\n\n```python\nprint(dm.col.shape)\ndm.mdim_col = MultiDimensionalColumn(shape=(2, 4))\nprint(dm.mdim_col.shape)\n```\n\nThe `loaded` property indicates whether a column is currently stored in memory, or whether it is offloaded to disk. This is mainly relevant for multidimensional columns, which are [automatically offloaded to disk when memory runs low](%link:largedata%).\n\n```python\nprint(dm.mdim_col.loaded)\n```\n\n\n## Assigning\n\n### Assigning by index, multiple indices, or slice\n\nYou can assign a single value to one or more cells in various ways.\n\n```python\ndm = DataMatrix(length=4)\n# Create a new columm\ndm.col = ''\n# By index: assign to a single cell (at row 1)\ndm.col[1] = ':-)'\n# By a tuple (or other iterable) of multiple indices:\n# assign to cells at rows 0 and 2\ndm.col[0, 2] = ':P'\n# By slice: assign from row 1 until the end\ndm.col[2:] = ':D'\nprint(dm)\n```\n\nYou can also assign multiple values at once, provided that the to-be-assigned sequence is of the correct length.\n\n```python\n# Assign to the full column\ndm.col = 1, 2, 3, 4\n# Assign to two cells\ndm.col[0, 2] = 'a', 'b'\nprint(dm)\n```\n\n\n### Assigning to cells that match a selection criterion\n\nAs will be described in more detail later on, comparing a column to a value gives a new `DataMatrix` that contains only the matching rows. This subsetted `DataMatrix` can in turn be used to assign to the matching rows of the original `DataMatrix`. This sounds a bit abstract but is very easy in practice:\n\n```python\ndm.col[1:] = ':D'\ndm.is_happy = 'no'\ndm.is_happy[dm.col == ':D'] = 'yes'\nprint(dm)\n```\n\n### Assigning to multidimensional columns\n\nAssigning to multidimensional columns works much the same as assigning to regular columns. The main differences are that there are multiple dimensions, and that dimensions can be named.\n\n\n```python\ndm = DataMatrix(length=2)\ndm.mdim_col = MultiDimensionalColumn(shape=(('x', 'y'), 3))\n# Set all values to a single value\ndm.mdim_col = 1\n# Set all last dimensions to a single array of shape 3\ndm.mdim_col = [ 1,  2,  3]\n# Set all rows to a single array of shape (2, 3)\ndm.mdim_col = [[ 1,  2,  3],\n               [ 4,  5,  6]]\n# Set the column to an array of shape (2, 3, 3)\ndm.mdim_col = [[[ 1,  2,  3],\n                [ 4,  5,  6]],\n               [[ 7,  8,  9],\n                [10, 11, 12]]]\n```\n\nTo assign to dimensions by name:\n\n```python\ndm.mdim_col[:, 'x'] = 1, 2, 3  # identical to assigning to dm.mdim_col[:, 0]\ndm.mdim_col[:, 'y'] = 4, 5, 6  # identical to assigning to dm.mdim_col[:, 1]\n```\n\n*Pro-tip:* When assigning an array-like object to a multidimensional column, the shape of the to-be-assigned array needs to match the final part of the shape of the column. This means that you can assign a (2, 3) array to a (2, 2, 3) column in which case all rows (the first dimension) are set to the array. shape However, you *cannot* assign a (2, 2) array to a (2, 2, 3) column.\n\n## Accessing\n\n### Accessing by index, multiple indices, or slice\n\n```python\ndm = DataMatrix(length=4)\n# Create a new column\ndm.col = 'a', 'b', 'c', 'd'\n# By index: select a single cell (at row 1).\nprint(dm.col[1])\n# By a tuple (or other iterable) of multiple indices:\n# select cells at rows 0 and 2. This gives a new column.\nprint(dm.col[0, 2])\n# By slice: assign from row 1 until the end. This gives a new column.\nprint(dm.col[2:])\n```\n\n\n### Accessing and averaging (ellipsis averaging) multidimensional columns\n\nAccessing multidimensional columns works much the same as accessing regular columns. The main differences are that there are multiple dimensions, and that dimensions can be named.\n\n```python\ndm = DataMatrix(length=2)\ndm.mdim_col = MultiDimensionalColumn(shape=(('x', 'y'), 3))\ndm.mdim_col = [[[ 1,  2,  3],\n                [ 4,  5,  6]],\n               [[ 7,  8,  9],\n                [10, 11, 12]]]\n# From all rows, get index 1 (named 'y') from the second dimension and index 2 from the third dimension.\nprint(dm.mdim_col[:, 'y', 2])\n```\n\nYou can select the average of a column using the ellipsis (`...`) index. For regular columns, this is indentical to accessing the `mean` property:\n\n```python\ndm.col = 1, 2\nprint(dm.col[...])  # identical to `dm.col.mean`\n```\n\nEllipsis averaging (`...`) is especially useful when working with multidimensional data, in which case it allows you to average over specific dimensions. As long as you don't average over the first dimension, which corresponds to the rows of the `DataMatrix`, the result is a new column.\n\n\n```python\n\n# Averaging over the third dimension gives a column of shape (2, 2)\ndm.avg3 = dm.mdim_col[:, :, ...]\n# Average over the second dimension gives a colum of shape (2, 3)\ndm.avg2 = dm.mdim_col[:, ...]\n# Averaging over the second and third dimensions gives a `FloatColumn`.\ndm.avg23 = dm.mdim_col[:, ..., ...]\nprint(dm)\n```\n\nWhen averaging over the first dimension, which corresponds to the rows of the `DataMatrix`, the result is either an array or (if all dimensions are averaged) a float:\n\n```python\n# Averaging over the rows gives an array of shape (2, 3)\nprint(dm.mdim_col[...])\n# Averaging over all dimensions gives a float\nprint(dm.mdim_col[..., ..., ...])\n```\n\n\n## Selecting\n\n### Selecting by column values\n\nYou can select by directly comparing columns to values. This returns a new `DataMatrix` object with only the selected rows.\n\n\n```python\ndm = DataMatrix(length=10)\ndm.col = range(10)\ndm_subset = dm.col > 5\nprint(dm_subset)\n```\n\n### Selecting by multiple criteria with `|` (or), `&` (and), and `^` (xor)\n\nYou can select by multiple criteria using the `|` (or), `&` (and), and `^` (xor) operators (but not the actual words 'and' and 'or'). Note the parentheses, which are necessary because `|`, `&`, and `^` have priority over other operators.\n\n\n```python\ndm_subset = (dm.col < 1) | (dm.col > 8)\nprint(dm_subset)\n```\n\n\n```python\ndm_subset = (dm.col > 1) & (dm.col < 8)\nprint(dm_subset)\n```\n\n### Selecting by multiple criteria by comparing to a set `{}`\n\nIf you want to check whether column values are identical to, or different from, a set of test values, you can compare the column to a `set` object. (This is considerably faster than comparing the column values to each of the test values separately, and then merging the result using `&` or `|`.)\n\n\n```python\ndm_subset = dm.col == {1, 3, 5, 7}\nprint(dm_subset)\n```\n\n### Selecting (filtering) with a function or lambda expression\n\nYou can also use a function or `lambda` expression to select column values. The function must take a single argument and its return value determines whether the column value is selected. This is analogous to the classic `filter()` function.\n\n\n```python\ndm_subset = dm.col == (lambda x: x % 2)\nprint(dm_subset)\n```\n\n### Selecting values that match another column (or sequence)\n\nYou can also select by comparing a column to a sequence, in which case a row-by-row comparison is done. This requires that the sequence has the same length as the column, is not a `set` object (because `set` objects are treated as described above).\n\n\n```python\ndm = DataMatrix(length=4)\ndm.col = 'a', 'b', 'c', 'd'\ndm_subset = dm.col == ['a', 'b', 'x', 'y']\nprint(dm_subset)\n```\n\n### Selecting values by type\n\nWhen a column contains values of different types, you can also select values by type:\n\n\n```python\ndm = DataMatrix(length=4)\ndm.col = 'a', 1, 'c', 2\ndm_subset = dm.col == int\nprint(dm_subset)\n```\n\n### Getting indices for rows that match selection criteria ('where')\n\nYou can get the indices for rows that match certain selection criteria by slicing a `DataMatrix` with a subset of itself. This is similar to the `numpy.where()` function.\n\n```python\ndm = DataMatrix(length=4)\ndm.col = 1, 2, 3, 4\nindices = dm[(dm.col > 1) & (dm.col < 4)]\nprint(indices)\n```\n\n### Selecting a subset of columns\n\nYou can select a subset of columns by passing the columns as an index to `dm[]`. Columns can be specified by name ('col3') or by object (`dm.col1`).\n\n```python\ndm = DataMatrix(length=4)\ndm.col1 = '\u263a'\ndm.col2 = 'a'\ndm.col3 = 1\ndm_subset = dm[dm.col1, 'col3']\nprint(dm_subset)\n```\n\n\n## Element-wise column operations\n\n### Multiplication, addition, etc.\n\nYou can apply basic mathematical operations on all cells in a column simultaneously. Cells with non-numeric values are ignored, except by the `+` operator, which then results in concatenation.\n\n```python\ndm = DataMatrix(length=3)\ndm.col = 0, 'a', 20\ndm.col2 = dm.col * .5\ndm.col3 = dm.col + 10\ndm.col4 = dm.col - 10\ndm.col5 = dm.col / 50\nprint(dm)\n```\n\n### Applying (mapping) a function or lambda expression\n\nYou can apply a function or `lambda` expression to all cells in a column simultaneously with the `@` operator. This analogous to the classic `map()` function.\n\n\n```python\ndm = DataMatrix(length=3)\ndm.col = 0, 1, 2\ndm.col2 = dm.col @ (lambda x: x*2)\nprint(dm)\n```\n\n## Iterating over rows, columns, and cells (for loops)\n\nBy iterating directly over a `DataMatrix` object, you get successive `Row` objects. From a `Row` object, you can directly access cells.\n\n\n```python\ndm.col = 'a', 'b', 'c'\nfor row in dm:\n    print(row)\n    print(row.col)\n```\n\nBy iterating over `DataMatrix.columns`, you get successive `(column_name, column)` tuples.\n\n\n```python\nfor colname, col in dm.columns:\n    print('%s = %s' % (colname, col))\n```\n\nBy iterating over a column, you get successive cells:\n\n\n```python\nfor cell in dm.col:\n    print(cell)\n```\n\nBy iterating over a `Row` object, you get (`column_name, cell`) tuples:\n\n\n```python\nrow = dm[0] # Get the first row\nfor colname, cell in row:\n    print('%s = %s' % (colname, cell))\n```\n\nThe `column_names` property gives a sorted list of all column names (without the corresponding column objects):\n\n\n```python\nprint(dm.column_names)\n```\n\n\n## Miscellanous notes\n\n### Type conversion and character encoding\n\nFor `MixedColumn`:\n\n- The strings 'nan', 'inf', and '-inf' are converted to the corresponding `float` values (`NAN`, `INF`, and `-INF`).\n- Byte-string values (`bytes`) are automatically converted to `str` assuming `utf-8` encoding.\n- Trying to assign an unsupported type results in a `TypeError`.\n- The string 'None' is *not* converted to the type `None`.\n\n\nFor `FloatColumn`:\n\n- The strings 'nan', 'inf', and '-inf' are converted to the corresponding `float` values (`NAN`, `INF`, and `-INF`).\n- Unsupported types are converted to `NAN`. A warning is shown.\n\n\nFor `IntColumn`:\n\n- Trying to assign non-`int` values results in a `TypeError`.\n\n\n### NAN and INF values\n\nYou have to take special care when working with `nan` data. In general, `nan` is not equal to anything else, not even to itself: `nan != nan`. You can see this behavior when selecting data from a `FloatColumn` with `nan` values in it.\n\n```python\nfrom datamatrix import DataMatrix, FloatColumn, NAN\ndm = DataMatrix(length=3)\ndm.f = FloatColumn\ndm.f = 0, NAN, 1\ndm = dm.f == [0, NAN, 1]\nprint(dm)\n```\n\nHowever, for convenience, you can select all `nan` values by comparing a `FloatColumn` to a single `nan` value:\n\n```python\ndm = DataMatrix(length=3)\ndm.f = FloatColumn\ndm.f = 0, NAN, 1\nprint(dm.f == NAN)\nprint('NaN values')\nprint('Non-NaN values')\nprint(dm.f != NAN)\n```",
    "title": "Basic use",
    "url": "https://pydatamatrix.eu/1.0/basic",
    "path": "content/pages/basic.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "# Working with large data (dynamic loading)\n\ntitle: Working with large data (dynamic loading)\n\n\n[TOC]\n\n\n## Dynamic loading of large data\n\nWhen working with large datasets, especially those containing multidimensional data, the available memory easily becomes a limiting factor. For example, a multidimensional column of shape `(2000, 500, 500)` takes 3.7 Gb of memory.\n\nDataMatrix automatically offloads multidimensional columns to disk when memory is running low. Let's see how this works by creating a DataMatrix with a single column of shape `(2000, 500, 500)`. (The first dimension corresponds to the length of the `DataMatrix`.) On its own, this column easily fits in memory, and we can use the `loaded` property to verify that the column has indeed been loaded into memory.\n\n\n~~~python\nfrom datamatrix import DataMatrix, MultiDimensionalColumn\n\ndm = DataMatrix(length=2000)\ndm.large_data1 = MultiDimensionalColumn(shape=(500, 500))\nprint(f'large_data1 loaded: {dm.large_data1.loaded}')\n~~~\n\n__Output:__\n\n~~~\nlarge_data1 loaded: True\n~~~\n\n\nHowever, if we add another column of the same size, memory starts to run low. Therefore, the old column (`large_data1`) is offloaded to disk, while the newly created column (`large_data2`) is held in memory. This happens automatically. \n\n\n~~~python\ndm.large_data2 = MultiDimensionalColumn(shape=(500, 500))\nprint(f'large_data1 loaded: {dm.large_data1.loaded}')\nprint(f'large_data2 loaded: {dm.large_data2.loaded}')\n~~~\n\n__Output:__\n\n~~~\nlarge_data1 loaded: False\nlarge_data2 loaded: True\n~~~\n\n\nDataMatrix tries to keep the most recently used columns in memory, and offloads the least recently used columns to disk. Therefore, if we assign the value 0 to `large_data1`, this column gets loaded into memory, while `large_data2` is offloaded to disk.\n\n\n~~~python\nimport numpy as np\n\ndm.large_data1 = 0\nprint(f'large_data1 loaded: {dm.large_data1.loaded}')\nprint(f'large_data2 loaded: {dm.large_data2.loaded}')\n~~~\n\n__Output:__\n\n~~~\nlarge_data1 loaded: True\nlarge_data2 loaded: False\n~~~\n\n\nYou can also manually force columns to be loaded into memory or offloaded to disk by changing the `loaded` property.\n\n\n~~~python\ndm.large_data1.loaded = False\nprint(f'large_data1 loaded: {dm.large_data1.loaded}')\nprint(f'large_data2 loaded: {dm.large_data2.loaded}')\n~~~\n\n__Output:__\n\n~~~\nlarge_data1 loaded: False\nlarge_data2 loaded: False\n~~~\n\n## Individual column sizes should not exceed available memory\n\nDynamic loading works best when columns do not, by themselves, exceed the available memory, even though the total size of the `DataMatrix` may exceed the available memory. For example, dynamic loading works well for a 16 Gb system when working with a `DataMatrix` that consists of 3 multidimensional columns of 8 Gb. Here, the total size of the `DataMatrix` is 3 \u00d7 4 = 24 Gb, which exceeds the 16 Gb of available memory; however, each column on its own is only 8 Gb, which does not exceed the available memory.\n\nIt is aso possible (though not recommended) to create columns that, by themselves, exceed the available memory, such as a 24 Gb column on a 16 Gb system. However, many numerical operations, such as taking the mean or standard deviation, will cause all data to be loaded into memory, thus causing Python to crash due to insufficient memory.\n\n\n## Implementation details\n\nWhen a column is offloaded to disk, a `numpy.memmap` object is created instead of a regular `numpy.ndarray`. This object is mapped onto a hidden temporary file in the current working directory. Depending on the operating system, this temporary file is either invisible (unlinked) or has the extension `.memmap`.\n\nSee also:\n\n- <https://numpy.org/doc/stable/reference/generated/numpy.memmap.html>",
    "title": "Working with large data (dynamic loading)",
    "url": "https://pydatamatrix.eu/1.0/largedata",
    "path": "content/pages/largedata.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "# datamatrix.multidimensional\n\ntitle: datamatrix.multidimensional\n\n\nThis module is typically imported as `mdim` for brevity:\n\n```python\nfrom datamatrix import multidimensional as mdim\n```\n\n\n[TOC]\n\n## What are multidimensional columns?\n\nA `MultiDimensionalColumn` is a column that itself has a shape; that is, each cell is itself an array. This allows you to represent multidimensional data, such as images and time series.\n\n\n<div class=\" YAMLDoc\" id=\"\" markdown=\"1\">\n\n \n\n<div class=\"FunctionDoc YAMLDoc\" id=\"flatten\" markdown=\"1\">\n\n## function __flatten__\\(dm\\)\n\nFlattens all multidimensional columns of a datamatrix to float columns.\nThe result is a new datamatrix where each row of the original\ndatamatrix is repeated for each value of the multidimensional column.\nThe new datamatrix does not contain any multidimensional columns.\n\nThis function requires that all multidimensional columns in `dm` have\nthe same shape, or that `dm` doesn't contain any multidimensional\ncolumns, in which case a copy of `dm` is returned.\n\n*Version note:* Moved to `datamatrix.multidimensional` in 1.0.0\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, MultiDimensionalColumn,              multidimensional as mdim\n\n dm = DataMatrix(length=2)\n dm.col = 'a', 'b'\n dm.m1 = MultiDimensionalColumn(shape=(3,))\n dm.m1[:] = 1,2,3\n dm.m2 = MultiDimensionalColumn(shape=(3,))\n dm.m2[:] = 3,2,1\n flat_dm = mdim.flatten(dm)\n print('Original:')\n print(dm)\n print('Flattened:')\n print(flat_dm)\n--%\n\n__Arguments:__\n\n- `dm` -- A DataMatrix\n\t- Type: DataMatrix\n\n__Returns:__\n\nA 'flattened' DataMatrix without multidimensional columns\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"infcount\" markdown=\"1\">\n\n## function __infcount__\\(col\\)\n\nCounts the number of `INF` values for each cell in a multidimensional\ncolumn, and returns this as an int column.\n\n*Version note:* Moved to `datamatrix.multidimensional` in 1.0.0\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, MultiDimensionalColumn,              multidimensional as mdim, INF\n\n dm = DataMatrix(length=3)\n dm.m = MultiDimensionalColumn(shape=(3,))\n dm.m[0] = 1, 2, 3\n dm.m[1] = 1, 2, INF\n dm.m[2] = INF, INF, INF\n dm.nr_of_inf = mdim.infcount(dm.m)\n print(dm)\n--%\n\n__Arguments:__\n\n- `col` -- A multidimensional column to count the `INF` values in.\n\t- Type: MultiDimensionalColumn\n\n__Returns:__\n\nAn int column with the number of `INF` values in each cell.\n\n- Type: IntColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"nancount\" markdown=\"1\">\n\n## function __nancount__\\(col\\)\n\nCounts the number of `NAN` values for each cell in a multidimensional\ncolumn, and returns this as an int column.\n\n*Version note:* Moved to `datamatrix.multidimensional` in 1.0.0\n\n*Version note:* New in 0.15.0\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, MultiDimensionalColumn,              multidimensional as mdim, NAN\n\n dm = DataMatrix(length=3)\n dm.m = MultiDimensionalColumn(shape=(3,))\n dm.m[0] = 1, 2, 3\n dm.m[1] = 1, 2, NAN\n dm.m[2] = NAN, NAN, NAN\n dm.nr_of_nan = mdim.nancount(dm.m)\n print(dm)\n--%\n\n__Arguments:__\n\n- `col` -- A column to count the `NAN` values in.\n\t- Type: MultiDimensionalColumn\n\n__Returns:__\n\nAn int column with the number of `NAN` values in each cell.\n\n- Type: IntColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"reduce\" markdown=\"1\">\n\n## function __reduce__\\(col, operation=<function nanmean at 0x7f31341e1090>\\)\n\nTransforms multidimensional values to single values by applying an\noperation (typically a mean) to each multidimensional value.\n\n*Version note:* Moved to `datamatrix.multidimensional` in 1.0.0\n\n*Version note:* As of 0.11.0, the function has been renamed to\n`reduce()`. The original `reduce_()` is deprecated.\n\n__Example:__\n\n%--\npython: |\n import numpy as np\n from datamatrix import DataMatrix, MultiDimensionalColumn,              multidimensional as mdim\n\n dm = DataMatrix(length=5)\n dm.m = MultiDimensionalColumn(shape=(3, 3))\n dm.m = np.random.random((5, 3, 3))\n dm.mean_y = mdim.reduce(dm.m)\n print(dm)\n--%\n\n__Arguments:__\n\n- `col` -- The column to reduce.\n\t- Type: MultiDimensionalColumn\n\n__Keywords:__\n\n- `operation` -- The operation function to use for the reduction. This function should accept `col` as first argument, and `axis=1` as keyword argument.\n\t- Default: <function nanmean at 0x7f31341e1090>\n\n__Returns:__\n\nA reduction of the signal.\n\n- Type: FloatColumn\n\n</div>\n\n</div>\n\n",
    "title": "datamatrix.multidimensional",
    "url": "https://pydatamatrix.eu/1.0/multidimensional",
    "path": "content/pages/multidimensional.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "# Working with series\n\ntitle: Working with series\n\nThis page has been deprecated. For information about series columns, see:\n\n- %link:basic%\n- <https://pythontutorials.eu/numerical/time-series/>",
    "title": "Working with series",
    "url": "https://pydatamatrix.eu/1.0/series-tutorial",
    "path": "content/pages/series-tutorial.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "# datamatrix.operations\n\ntitle: datamatrix.operations\n\nA set of common operations that can be apply to columns and `DataMatrix` objects. This module is typically imported as `ops` for brevity:\n\n```python\nfrom datamatrix import operations as ops\n```\n\n\n[TOC]\n\n<div class=\" YAMLDoc\" id=\"\" markdown=\"1\">\n\n \n\n<div class=\"FunctionDoc YAMLDoc\" id=\"auto_type\" markdown=\"1\">\n\n## function __auto\\_type__\\(dm\\)\n\n*Requires fastnumbers*\n\nConverts all columns of type MixedColumn to IntColumn if all values are\ninteger numbers, or FloatColumn if all values are non-integer numbers.\n\n%--\npython: |\n from datamatrix import DataMatrix, operations as ops\n\n dm = DataMatrix(length=5)\n dm.A = 'a'\n dm.B = 1\n dm.C = 1.1\n dm_new = ops.auto_type(dm)\n print('dm_new.A: %s' % type(dm_new.A))\n print('dm_new.B: %s' % type(dm_new.B))\n print('dm_new.C: %s' % type(dm_new.C))\n--%\n\n__Arguments:__\n\n- `dm` -- No description\n\t- Type: DataMatrix\n\n__Returns:__\n\nNo description\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"bin_split\" markdown=\"1\">\n\n## function __bin\\_split__\\(col, bins\\)\n\nSplits a DataMatrix into bins; that is, the DataMatrix is first sorted\nby a column, and then split into equal-size (or roughly equal-size)\nbins.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, operations as ops\n\n dm = DataMatrix(length=5)\n dm.A = 1, 0, 3, 2, 4\n dm.B = 'a', 'b', 'c', 'd', 'e'\n for bin, dm in enumerate(ops.bin_split(dm.A, bins=3)):\n    print('bin %d' % bin)\n    print(dm)\n--%\n\n__Arguments:__\n\n- `col` -- The column to split by.\n\t- Type: BaseColumn\n- `bins` -- The number of bins.\n\t- Type: int\n\n__Returns:__\n\nA generator that iterates over the bins.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"fullfactorial\" markdown=\"1\">\n\n## function __fullfactorial__\\(dm, ignore=u''\\)\n\n*Requires numpy*\n\nCreates a new DataMatrix that uses a specified DataMatrix as the base\nof a full-factorial design. That is, each value of every row is \ncombined with each value from every other row. For example:\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, operations as ops\n\n dm = DataMatrix(length=2)\n dm.A = 'x', 'y'\n dm.B = 3, 4\n dm = ops.fullfactorial(dm)\n print(dm)\n--%\n\n__Arguments:__\n\n- `dm` -- The source DataMatrix.\n\t- Type: DataMatrix\n\n__Keywords:__\n\n- `ignore` -- A value that should be ignored.\n\t- Default: ''\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"group\" markdown=\"1\">\n\n## function __group__\\(dm, by\\)\n\n*Requires numpy*\n\nGroups the DataMatrix by unique values in a set of grouping columns.\nGrouped columns are stored as SeriesColumns. The columns that are\ngrouped should contain numeric values. The order in which groups appear\nin the grouped DataMatrix is unpredictable.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, operations as ops\n\n dm = DataMatrix(length=4)\n dm.A = 'x', 'x', 'y', 'y'\n dm.B = 0, 1, 2, 3\n print('Original:')\n print(dm)\n dm = ops.group(dm, by=dm.A)\n print('Grouped by A:')\n print(dm)\n--%\n\n__Arguments:__\n\n- `dm` -- The DataMatrix to group.\n\t- Type: DataMatrix\n- `by` -- A column or list of columns to group by.\n\t- Type: BaseColumn, list\n\n__Returns:__\n\nA grouped DataMatrix.\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"keep_only\" markdown=\"1\">\n\n## function __keep\\_only__\\(dm, \\*cols\\)\n\nRemoves all columns from the DataMatrix, except those listed in `cols`.\n\n*Version note:* As of 0.11.0, the preferred way to select a subset of\ncolumns is using the `dm = dm[('col1', 'col2')]` notation.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, operations as ops\n\n dm = DataMatrix(length=5)\n dm.A = 'a', 'b', 'c', 'd', 'e'\n dm.B = range(5)\n dm.C = range(5, 10)\n dm_new = ops.keep_only(dm, dm.A, dm.C)\n print(dm_new)\n--%\n\n__Arguments:__\n\n- `dm` -- No description\n\t- Type: DataMatrix\n\n__Argument list:__\n\n- `*cols`: A list of column names, or column objects.\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"pivot_table\" markdown=\"1\">\n\n## function __pivot\\_table__\\(dm, values, index, columns, \\*args, \\*\\*kwargs\\)\n\n*Requires pandas*\n\n*Version note:* New in 0.14.1\n\nCreates a pivot table where rows correspond to levels of `index`,\ncolumns correspond to levels of `columns`, and cells contain aggregate\nvalues of `values`.\n\nA typical use for a pivot table is to create a summary report for a\ndata set. For example, in an experiment where reaction times of human\nparticipants were measured on a large number of trials under different\nconditions, each row might correspond to one participant, each column\nto an experimental condition (or a combination of experimental\nconditions), and cells might contain mean reaction times.\n\nThis function is a wrapper around the `pandas.pivot_table()`. For an\noverview of possible `*args` and `**kwargs`, see\n[this page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html).\n\n__Example:__\n\n%--\npython: |\n from datamatrix import operations as ops, io\n\n dm = io.readtxt('data/fratescu-replication-data-exp1.csv')\n pm = ops.pivot_table(dm, values=dm.RT_search, index=dm.subject_nr,\n                      columns=dm.load)\n print(pm)\n--%\n\n__Arguments:__\n\n- `dm` -- The source DataMatrix.\n\t- Type: DataMatrix\n- `values` -- A column or list of columns to aggregate.\n\t- Type: BaseColumn, str, list\n- `index` -- A column or list of columns to separate rows by.\n\t- Type: BaseColumn, str, list\n- `columns` -- A column or list of columns to separate columns by.\n\t- Type: BaseColumn, str, list\n\n__Argument list:__\n\n- `*args`: No description.\n\n__Keyword dict:__\n\n- `**kwargs`: No description.\n\n__Returns:__\n\nNo description\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"random_sample\" markdown=\"1\">\n\n## function __random\\_sample__\\(obj, k\\)\n\n*New in v0.11.0*\n\nTakes a random sample of `k` rows from a DataMatrix or column. The\norder of the rows in the returned DataMatrix is random.\n\n__Example:__\n\n```python\nfrom datamatrix import DataMatrix, operations as ops\n\ndm = DataMatrix(length=5)\ndm.A = 'a', 'b', 'c', 'd', 'e'\ndm = ops.random_sample(dm, k=3)\nprint(dm)\n```\n\n__Arguments:__\n\n- `obj` -- No description\n\t- Type: DataMatrix, BaseColumn\n- `k` -- No description\n\t- Type: int\n\n__Returns:__\n\nA random sample from a DataMatrix or column.\n\n- Type: DataMatrix, BaseColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"replace\" markdown=\"1\">\n\n## function __replace__\\(col, mappings=\\{\\}\\)\n\nReplaces values in a column by other values.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, operations as ops\n\n dm = DataMatrix(length=3)\n dm.old = 0, 1, 2\n dm.new = ops.replace(dm.old, {0 : 'a', 2 : 'c'})\n print(dm_new)\n--%\n\n__Arguments:__\n\n- `col` -- The column to weight by.\n\t- Type: BaseColumn\n\n__Keywords:__\n\n- `mappings` -- A dict where old values are keys and new values are values.\n\t- Type: dict\n\t- Default: {}\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"shuffle\" markdown=\"1\">\n\n## function __shuffle__\\(obj\\)\n\nShuffles a DataMatrix or a column. If a DataMatrix is shuffled, the\norder of the rows is shuffled, but values that were in the same row\nwill stay in the same row.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, operations as ops\n\n dm = DataMatrix(length=5)\n dm.A = 'a', 'b', 'c', 'd', 'e'\n dm.B = ops.shuffle(dm.A)\n print(dm)\n--%\n\n__Arguments:__\n\n- `obj` -- No description\n\t- Type: DataMatrix, BaseColumn\n\n__Returns:__\n\nThe shuffled DataMatrix or column.\n\n- Type: DataMatrix, BaseColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"shuffle_horiz\" markdown=\"1\">\n\n## function __shuffle\\_horiz__\\(\\*obj\\)\n\nShuffles a DataMatrix, or several columns from a DataMatrix,\nhorizontally. That is, the values are shuffled between columns from the\nsame row.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, operations as ops\n\n dm = DataMatrix(length=5)\n dm.A = 'a', 'b', 'c', 'd', 'e'\n dm.B = range(5)\n dm = ops.shuffle_horiz(dm.A, dm.B)\n print(dm)\n--%\n\n__Argument list:__\n\n- `*desc`: A list of BaseColumns, or a single DataMatrix.\n- `*obj`: No description.\n\n__Returns:__\n\nThe shuffled DataMatrix.\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"sort\" markdown=\"1\">\n\n## function __sort__\\(obj, by=None\\)\n\nSorts a column or DataMatrix. In the case of a DataMatrix, a column\nmust be specified to determine the sort order. In the case of a column,\nthis needs to be specified if the column should be sorted by another\ncolumn.\n\nThe sort order is as follows:\n\n- `-INF`\n- `int` and `float` values in increasing order\n- `INF`\n- `str` values in alphabetical order, where uppercase letters come\n  first\n- `None`\n- `NAN`\n\nYou can also sort columns (but not DataMatrix objects) using the\nbuilt-in `sorted()` function. However, when sorting different mixed\ntypes, this may lead to Exceptions or (in the case of `NAN` values)\nunpredictable results.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, operations as ops\n\n dm = DataMatrix(length=3)\n dm.A = 2, 0, 1\n dm.B = 'a', 'b', 'c'\n dm = ops.sort(dm, by=dm.A)\n print(dm)\n--%\n\n__Arguments:__\n\n- `obj` -- No description\n\t- Type: DataMatrix, BaseColumn\n\n__Keywords:__\n\n- `by` -- The sort key, that is, the column that is used for sorting the DataMatrix, or the other column.\n\t- Type: BaseColumn\n\t- Default: None\n\n__Returns:__\n\nThe sorted DataMatrix, or the sorted column.\n\n- Type: DataMatrix, BaseColumn\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"split\" markdown=\"1\">\n\n## function __split__\\(col, \\*values\\)\n\nSplits a DataMatrix by unique values in a column.\n\n*Version note:* As of 0.12.0, `split()` accepts multiple columns as\nshown below.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, operations as ops\n\n dm = DataMatrix(length=4)\n dm.A = 0, 0, 1, 1\n dm.B = 'a', 'b', 'c', 'd'\n # If no values are specified, a (value, DataMatrix) iterator is\n # returned.\n print('Splitting by a single column')\n for A, sdm in ops.split(dm.A):\n     print('sdm.A = %s' % A)\n     print(sdm)\n # You can also split by multiple columns at the same time.\n print('Splitting by two columns')\n for A, B, sdm in ops.split(dm.A, dm.B):\n     print('sdm.A = %s, sdm.B = %s' % (A, B))\n # If values are specific an iterator over DataMatrix objects is\n # returned.\n print('Splitting by values')\n dm_a, dm_c = ops.split(dm.B, 'a', 'c')\n print('dm.B == \"a\"')\n print(dm_a)\n print('dm.B == \"c\"')\n print(dm_c)\n--%\n\n__Arguments:__\n\n- `col` -- The column to split by.\n\t- Type: BaseColumn\n\n__Argument list:__\n\n- `*values`: Splits the DataMatrix based on these values. If this is provided, an iterator over DataMatrix objects is returned, rather than an iterator over (value, DataMatrix) tuples.\n\n__Returns:__\n\nA iterator over (value, DataMatrix) tuples if no values are provided; an iterator over DataMatrix objects if values are provided.\n\n- Type: Iterator\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"stack\" markdown=\"1\">\n\n## function __stack__\\(\\*dms\\)\n\nStacks multiple DataMatrix objects such that the resulting DataMatrix\nhas a length that is equal to the sum of all the stacked DataMatrix\nobjects. Phrased differently, this function vertically concatenates\nDataMatrix objects.\n\nSee also [`stack_multiprocess()`](%url:functional%) for stacking\nDataMatrix objects that are returned by functions running in different\nprocesses.\n\nStacking two DataMatrix objects can also be done with the `<<`\noperator. However, when stacking more than two DataMatrix objects,\nusing `stack()` is much faster than iteratively stacking with `<<`.\n\n*Version note:* New in 1.0.0\n\n__Example:__\n\n%--\npython: |\n from datamatrix import operations as ops\n\n dm1 = DataMatrix(length=2)\n dm1.col = 'A'\n dm2 = DataMatrix(length=2)\n dm2.col = 'B'\n dm3 = DataMatrix(length=2)\n dm3.col = 'C'\n dm = ops.stack(dm1, dm2, dm3)\n print(dm)\n--%\n\n__Argument list:__\n\n- `*dms`: OrderedDict([('desc', 'A list of DataMatrix objects.'), ('type', 'list')])\n\n__Returns:__\n\nNo description\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"weight\" markdown=\"1\">\n\n## function __weight__\\(col\\)\n\nWeights a DataMatrix by a column. That is, each row from a DataMatrix\nis repeated as many times as the value in the weighting column.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, operations as ops\n\n dm = DataMatrix(length=3)\n dm.A = 1, 2, 0\n dm.B = 'x', 'y', 'z'\n print('Original:')\n print(dm)\n dm = ops.weight(dm.A)\n print('Weighted by A:')\n print(dm)\n--%\n\n__Arguments:__\n\n- `col` -- The column to weight by.\n\t- Type: BaseColumn\n\n__Returns:__\n\nNo description\n\n- Type: DataMatrix\n\n</div>\n\n<div class=\"FunctionDoc YAMLDoc\" id=\"z\" markdown=\"1\">\n\n## function __z__\\(col\\)\n\nTransforms a column into z scores such that the mean of all values is\n0 and the standard deviation is 1.\n\n*Version note:* As of 0.13.2, `z()` returns a `FloatColumn` when a\nregular column is give. For non-numeric values, the z score is NAN. If\nthe standard deviation is 0, z scores are also NAN.\n\n*Version note:* As of 0.15.3, `z()` also accepts series columns, in\nwhich case the series is z-transformed such that the grand mean of\nall samples is 0, and the grand standard deviation of all samples is\n1.\n\n__Example:__\n\n%--\npython: |\n from datamatrix import DataMatrix, operations as ops\n\n dm = DataMatrix(length=5)\n dm.col = range(5)\n dm.z = ops.z(dm.col)\n print(dm)\n--%\n\n__Arguments:__\n\n- `col` -- The column to transform.\n\t- Type: BaseColumn\n\n__Returns:__\n\nNo description\n\n- Type: BaseColumn\n\n</div>\n\n</div>\n\n",
    "title": "datamatrix.operations",
    "url": "https://pydatamatrix.eu/1.0/operations",
    "path": "content/pages/operations.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "# Page not found\n\ntitle: Page not found\n\nThe page that you requested does not exist. Perhaps it has moved.\n\nTo find the information you're looking for:\n\n- Browse the menu at the top of this page; or\n- Search using the search bar at the left of this page.",
    "title": "Page not found",
    "url": "https://pydatamatrix.eu/1.0/notfound",
    "path": "content/pages/notfound.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "# DataMatrix\n\ntitle: DataMatrix\n\n`DataMatrix` is an intuitive Python library for working with column-based, time-series, and multidimensional data. It's a light-weight and easy-to-use alternative to `pandas`.\n\n<div class=\"btn-group\" role=\"group\" aria-label=\"...\">\n  <a role=\"button\" class=\"btn btn-success\" href=\"%url:install%\">\n\t\t<span class=\"glyphicon glyphicon-download\" aria-hidden=\"true\"></span>\n\t\tInstall\n\t </a>\n  <a role=\"button\" class=\"btn btn-success\" href=\"%url:basic%\">\n  <span class=\"glyphicon glyphicon-education\" aria-hidden=\"true\"></span>\n  \tBasic use\n  </a>\n  <a role=\"button\" class=\"btn btn-success\" href=\"https://professional.cogsci.nl/\">\n  <span class=\"glyphicon glyphicon-comment\" aria-hidden=\"true\"></span>\n  Get support</a>\n</div>\n\n\n## Features\n\n- [An intuitive syntax](%link:basic%) that makes your code easy to read\n- Mix tabular data with [time series](%link:series%) and [multidimensional data](%link:multidimensional) in a single data structure\n- Support for [large data](%link:largedata%) by intelligent (and automatic) offloading of data to disk when memory is running low\n- Advanced [memoization (caching)](%link:memoization%)\n- Requires only the Python standard libraries (but you can use `numpy` to improve performance)\n- Compatible with your favorite data-science libraries:\n    - `seaborn` and `matplotlib` for [plotting](https://pythontutorials.eu/numerical/plotting)\n    - `scipy`, `statsmodels`, and `pingouin` for [statistics](https://pythontutorials.eu/numerical/statistics)\n    - `mne` for analysis of electroencephalographic (EEG) and magnetoencephalographic (MEG) data\n    - [Convert](%link:convert%) to and from `pandas.DataFrame`\n    - Looks pretty inside a Jupyter Notebook\n\n\n## Ultra-short cheat sheet\n\n~~~python\nfrom datamatrix import DataMatrix, io\n# Read a DataMatrix from file\ndm = io.readtxt('data.csv')\n# Create a new DataMatrix\ndm = DataMatrix(length=5)\n# The first two rows\nprint(dm[:2])\n# Create a new column and initialize it with the Fibonacci series\ndm.fibonacci = 0, 1, 1, 2, 3\n# You can also specify column names as if they are dict keys\ndm['fibonacci'] = 0, 1, 1, 2, 3\n# Remove 0 and 3 with a simple selection\ndm = (dm.fibonacci > 0) & (dm.fibonacci < 3)\n# Get a list of indices that match certain criteria\nprint(dm[(dm.fibonacci > 0) & (dm.fibonacci < 3)])\n# Select 1, 1, and 2 by matching any of the values in a set\ndm = dm.fibonacci == {1, 2}\n# Select all odd numbers with a lambda expression\ndm = dm.fibonacci == (lambda x: x % 2)\n# Change all 1s to -1\ndm.fibonacci[dm.fibonacci == 1] = -1\n# The first two cells from the fibonacci column\nprint(dm.fibonacci[:2])\n# Column mean\nprint('Mean: %s' % dm.fibonacci.mean)\n# Multiply all fibonacci cells by 2\ndm.fibonacci_times_two = dm.fibonacci * 2\n# Loop through all rows\nfor row in dm:\n    print(row.fibonacci) # get the fibonacci cell from the row\n# Loop through all columns\nfor colname, col in dm.columns:\n    for cell in col: # Loop through all cells in the column\n        print(cell) # do something with the cell\n# Or just see which columns exist\nprint(dm.column_names)\n~~~",
    "title": "DataMatrix",
    "url": "https://pydatamatrix.eu/1.0/index",
    "path": "content/pages/index.md",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "foundation": false,
    "howto": false,
    "chunk": 1,
    "total_chunks": 1
  },
  {
    "content": "\"\"\"\n# Instructions for Python DataMatrix\n\n- DataMatrix is an intuitive Python library for working with column-based, time-series, and multidimensional data.\n- DataMatrix is a light-weight and easy-to-use alternative to pandas.\n- Never use dm.column.mean() but dm.column[...] to get the mean\n\"\"\"\nfrom datamatrix import DataMatrix, io\n# Read a DataMatrix from file\ndm = io.readtxt('data.csv')\n# Create a new DataMatrix\ndm = DataMatrix(length=5)\n# The first two rows\nprint(dm[:2])\n# Create a new column and initialize it with the Fibonacci series\ndm.fibonacci = 0, 1, 1, 2, 3\n# You can also specify column names as if they are dict keys\ndm['fibonacci'] = 0, 1, 1, 2, 3\n# Remove 0 and 3 with a simple selection\ndm = (dm.fibonacci > 0) & (dm.fibonacci < 3)\n# Get a list of indices that match certain criteria\nprint(dm[(dm.fibonacci > 0) & (dm.fibonacci < 3)])\n# Select 1, 1, and 2 by matching any of the values in a set\ndm = dm.fibonacci == {1, 2}\n# Select all odd numbers with a lambda expression\ndm = dm.fibonacci == (lambda x: x % 2)\n# Change all 1s to -1\ndm.fibonacci[dm.fibonacci == 1] = -1\n# The first two cells from the fibonacci column\nprint(dm.fibonacci[:2])\n# Column mean\nprint(dm.fibonacci[...])\n# Multiply all fibonacci cells by 2\ndm.fibonacci_times_two = dm.fibonacci * 2\n# Loop through all rows\nfor row in dm:\n    print(row.fibonacci) # get the fibonacci cell from the row\n# Loop through all columns\nfor colname, col in dm.columns:\n    for cell in col: # Loop through all cells in the column\n        print(cell) # do something with the cell\n# Or just see which columns exist\nprint(dm.column_names)\n",
    "title": "Foundation document for datamatrix",
    "collection": "datamatrix",
    "topic": "datamatrix",
    "howto": false,
    "foundation": true
  },
  {
    "content": "# Python EyeLinkParser\n\nSebastiaan Math\u00f4t and contributors <br />\nCopyright 2016-2023  <br />\nhttp://www.cogsci.nl/smathot\n\n## About\n\nThe `python-eyelinkparser` module provides a framework to parse EyeLink data files in `.asc` format, that is, the format that you get after converting an `.edf` file with `edf2asc`. This module is mostly for personal use, and is not very well documented.\n\n## Installation\n\n```\npip install eyelinkparser\n```\n\n## Expected format\n\nThe parser assumes monocular recording.\n\n\n## Expected messages\n\nBy default, the parser assumes that particular messages are sent to the logfile. If you use different messages, you need to override functions in `_eyelinkparser.EyeLinkParser`. This is not explained here, but you can look in the source code to see how it works.\n\nTrial start:\n\n\tstart_trial [trialid]\n\t\nTrial end:\n\n\tend_trial\n\tstop_trial\n\t\nVariables:\n\n\tvar [name] [value]\n\t\nStart of a period of continuous data:\n\t\n\tstart_phase [name]\n\tphase [name]\n\t\nEnd of a period of continuous data:\n\n\tend_phase [name]\n\tstop_phase [name]\n\t\n\t\n\n## Function reference\n\n## <span style=\"color:purple\">eyelinkparser.EyeLinkParser</span>_(folder='data', ext=('.asc', '.edf', '.tar.xz'), downsample=None, maxtracelen=None, traceprocessor=None, phasefilter=None, phasemap={}, trialphase=None, edf2asc\\_binary='edf2asc', multiprocess=False, asc\\_encoding=None, pupil\\_size=True, gaze\\_pos=True, time\\_trace=True)_\n\nThe main parser class. This is generally not created directly, but\nthrough the `eyelinkparser.parse()` function, which takes the same keywords\nas the `EyeLinkParser` constructor (i.e. the keywords below).\n\n### Parameters\n\n* **folder: str, optional**\n\n  The folder that contains .edf or .asc data files, or files compressed\n  as .tar.xz archives.\n\n* **ext: str or tuple, optional**\n\n  Allowed file extensions, or a tuple of extensions, for data files.\n\n* **downsample: int or None, optional**\n\n  Indicates whether traces (if any) should be downsampled. For example, a\n  value of 10 means that the signal becomes 10 times shorter. Downsample\n  creates a simple traceprocessor, and can therefore not be used in\n  combination with the traceprocessor argument.\n\n* **maxtracelen: int or None, optional**\n\n  A maximum length for traces. Longer traces are truncated and a\n  `UserWarning` is emitted. This length refers to the trace after\n  downsampling/ processing.\n\n* **traceprocessor: callable or None, optional**\n\n  A function that is applied to each trace before the trace is written to\n  the SeriesColumn. This can be used to apply a series of operations that\n  are best done on the raw signal, such as first correcting blinks and\n  then downsampling the signal.\n\n  The function must accept two arguments: first a label for the trace,\n  which is 'pupil', 'xcoor', 'ycoor', or 'time'. This allows the function\n  to distinguish the different kinds of singals; second, the trace\n  itself.\n\n  See `eyelinkparser.defaulttraceprocessor` for a convenience function\n  that applies blink correction and downsampling.\n\n* **trialphase: str or None, optional**\n\n  Indicates the name of a phase that should be automatically started when\n  the trial starts, or `None` when no trial should be automatically\n  started. This is mostly convenient for processing trials that consist\n  of a single long epoch, or when no `start_phase` messages were written\n  to the log file.\n\n* **phasefilter: callable or None, optional**\n\n  A function that receives a phase name as argument, and returns a bool\n  indicating whether that phase should be retained.\n\n* **phasemap: dict, optional**\n\n  A dict in which keys are phase names that are renamed to the associated\n  values. This is mostly useful to merge subsequent traces, in which\n  case the key is the first trace and the value is the second trace.\n\n* **edf2asc\\_binary: str, optional**\n\n  The name of the edf2asc executable, which if available can be used to\n  automatically convert .edf files to .asc. If not available, the parser\n  can only parse .asc files.\n\n* **multiprocess: bool or int, optional**\n\n  Indicates whether each file should be processed in a different process.\n  This can speed up parsing considerably. If not `False`, this should be\n  an int to indicate the number of processes.\n\n* **asc\\_encoding: str or None, optional**\n\n  Indicates the character encoding of the `.asc` files, or `None` to use\n  system default.\n\n* **pupil\\_size: bool, optional**\n\n  Indicates whether pupil-size traces should be stored. If enabled, pupil\n  size is stored as `ptrace_[phase]` columns.\n\n* **gaze\\_pos: bool, optional**\n\n  Indicates whether horizontal and vertical gaze-position traces should\n  be stored. If enabled, gaze position is stored as `xtrace_[phase]` and\n  `ytrace_[phase]` columns.\n\n* **time\\_trace: bool, optional**\n\n  Indicates whether timestamp traces should be stored, which indicate the\n  timestamps of the corresponding pupil and gaze-position traces. If\n  enabled, timestamps are stored as `ptrace_[phase]` columns.\n\n### Examples\n\n```python\nimport eyelinkparser as ep\ndm = ep.parse(defaulttraceprocessor=ep.defaulttraceprocessor(\n    blinkreconstruct=True, downsample=True, mode='advanced'))\n```\n\n\n## Tutorial\n\nFor a tutorial about using EyeLinkParser, see:\n\n- <https://pydatamatrix.eu/eyelinkparser/>\n\n## License\n\n`python-eyelinkparser` is licensed under the [GNU General Public License\nv3](http://www.gnu.org/licenses/gpl-3.0.en.html).\n",
    "url": "https://github.com/open-cogsci/eyelinkparser",
    "collection": "datamatrix",
    "howto": false,
    "foundation": false,
    "topics": [
      "datamatrix"
    ]
  },
  {
    "content": "# Time Series Test\n\n*Statistical testing and plotting functions for time-series data in general, and data from cognitive-pupillometry and electroencephalography (EEG) experiments in particular. Based on linear mixed effects modeling (or regular multiple linear regression), crossvalidation, and cluster-based permutation testing.*\n\nSebastiaan Math\u00f4t (@smathot) <br />\nCopyright 2021 - 2024\n\n[![Publish to PyPi](https://github.com/smathot/time_series_test/actions/workflows/publish-package.yaml/badge.svg)](https://github.com/smathot/time_series_test/actions/workflows/publish-package.yaml)\n[![Tests](https://github.com/smathot/time_series_test/actions/workflows/run-unittests.yaml/badge.svg)](https://github.com/smathot/time_series_test/actions/workflows/run-unittests.yaml)\n\n\n## Contents\n\n- [Citation](#citation)\n- [About](#about)\n- [Dependencies](#dependencies)\n- [Usage](#usage)\n- [Function reference](#function-reference)\n- [License](#license)\n\n\n## Citation\n\nMath\u00f4t, S., & Vilotijevi\u0107, A. (2022). Methods in cognitive pupillometry: design, preprocessing, and analysis. *Behavior Research Methods*. <https://doi.org/10.1101/2022.02.23.481628>\n\n\n## About\n\nThis library provides two main functions for statistical testing of time-series data: `lmer_crossvalidation_test()` and `lmer_permutation_test()`. For a detailed description, see the manuscript above, but below a short introduction to both functions with their respective advantages and disadavantages.\n\n\n### When to use crossvalidation?\n\nIn general terms, `lmer_crossvalidation_test()` implements a statistical test for a specific-yet-common question when analyzing time-series data:\n\n> Do one or more independent variables affect a continuously recorded dependent variable (a 'time series') at any point in time?\n\nWhen to use this test:\n\n- For time series consisting of only a single component, that is, when each independent variable has only a single effect on the time series. An example of this is the effect of stimulus intensity on pupil size, when presenting light flashes of different intensities.\n- When you do not know a priori which time points to test.\n\nWhen *not* to use this test:\n\n- For time series that contain multiple components, that is, when each independent variable affects the time series in multiple ways that change over time. An example of this is the effect of visual attention on lateralized EEG recordings, where different EEG components emerge at different points in time.\n- When you know a priori which time points to test.\n\nMore specifically, `lmer_crossvalidation_test()` locates and statistically tests effects in time-series data. It does so by using crossvalidation to identify time points to test, and then using a linear mixed effects model to actually perform the statistical test. More specifically, the data is subdivided in a number of subsets (by default 4). It takes one of the subsets (the *test* set) out of the full dataset, and conducts a linear mixed effects model on each sample of the remaining data (the *training* set). The sample with the highest absolute z value in the training set is used as the sample-to-be-tested for the test set. This procedure is repeated for all subsets of the data, and for all fixed effects in the model. Finally, a single linear mixed effects model is conducted for each fixed effects on the samples that were thus identified.\n\nThis packages also provides a function (`plot()`) to visualize time-series data to visually annotate the results of `lmer_crossvalidation_test()`.\n\n\n### When to use `lmer_permutation_test()`?\n\n`lmer_permutation_test()` implements a fairly standard cluster-based permutation test, which differs from most other implementations in that it relies on linear mixed-effects modeling to calculate the test statistics. Therefore, this function tends to be extremely computationally intensive, but should also be more sensitive than cluster-based permutation tests that are based on average data. Its main advantage as compared to `lmer_crossvalidation_test()` is that it is also valid for data with multiple components, such as event-related potentials (ERPs).\n\n\n### Can the tests also be based on regular multiple regression (instead of linear mixed effects modeling)?\n\nYes. If you pass `groups=None` to any of the functions, the analysis will be based on a regular multiple linear regression instead of linear mixed effects modeling.\n\n\n## Installation\n\n```\npip install time_series_test\n```\n\n## Dependencies\n\n- [Python 3](https://www.python.org/)\n- [datamatrix](https://pydatamatrix.eu/)\n- [statsmodels](https://www.statsmodels.org/)\n- [matplotlib](https://matplotlib.org/)\n\n\n## Usage\n\nWe will use data from [Zhou, Lorist, and Math\u00f4t (2021)](https://doi.org/10.1101/2021.11.23.469689). In brief, this is data from a visual-working-memory experiment in which participant memorized one or more colors (set size: 1, 2, 3 or 4) of two different types (color type: proto, nonproto) while pupil size was being recorded during a 3s retention interval.\n\nThis dataset contains the following columns:\n\n- `pupil`, which is is our dependent measure. It is a baseline-corrected pupil time series of 300 samples, recorded at 100 Hz\n- `subject_nr`, which we will use as a random effect\n- `set_size`, which we will use as a fixed effect\n- `color_type`, which we will use as a fixed effect\n\nFirst, load the dataset:\n\n\n\n```python\nfrom datamatrix import io\ndm = io.readpickle('data/zhou_et_al_2021.pkl')\n```\n\n\n\nThe `plot()` function provides a convenient way to plot pupil size over time as a function of one or two factors, in this case set size and color type:\n\n\n\n```python\nimport time_series_test as tst\nfrom matplotlib import pyplot as plt\n\ntst.plot(dm, dv='pupil', hue_factor='set_size', linestyle_factor='color_type',\n         sampling_freq=100)\nplt.savefig('img/signal-plot-1.png')\n```\n\n\n\n![](https://github.com/smathot/time_series_test/raw/master/img/signal-plot-1.png)\n\nFrom this plot, we can tell that there appear to be effects in the 1500 to 2000 ms interval. To test this, we could perform a linear mixed effects model on this interval, which corresponds to samples 150 to 200.\n\nThe model below uses mean pupil size during the 150 - 200 sample range as dependent measure, set size and color type as fixed effects, and a random by-subject intercept. In the more familiar notation of the R package `lme4`, this corresponds to `mean_pupil ~ set_size * color_type + (1 | subject_nr)`. (To use more complex random-effects structures, you can use the `re_formula` argument to `mixedlm()`.)\n\n\n\n```python\nfrom statsmodels.formula.api import mixedlm\nfrom datamatrix import series as srs, NAN\n\ndm.mean_pupil = srs.reduce(dm.pupil[:, 150:200])\ndm_valid_data = dm.mean_pupil != NAN\nmodel = mixedlm(formula='mean_pupil ~ set_size * color_type',\n                data=dm_valid_data, groups='subject_nr').fit()\nprint(model.summary())\n```\n\n__Output:__\n``` .text\n                    Mixed Linear Model Regression Results\n=============================================================================\nModel:                    MixedLM       Dependent Variable:       mean_pupil \nNo. Observations:         7300          Method:                   REML       \nNo. Groups:               30            Scale:                    38610.3390 \nMin. group size:          235           Log-Likelihood:           -48952.3998\nMax. group size:          248           Converged:                Yes        \nMean group size:          243.3                                              \n-----------------------------------------------------------------------------\n                              Coef.   Std.Err.   z    P>|z|  [0.025   0.975] \n-----------------------------------------------------------------------------\nIntercept                    -144.024   17.438 -8.259 0.000 -178.202 -109.846\ncolor_type[T.proto]           -24.133   11.299 -2.136 0.033  -46.278   -1.987\nset_size                       49.979    2.906 17.200 0.000   44.284   55.675\nset_size:color_type[T.proto]   10.176    4.120  2.470 0.014    2.101   18.251\nsubject_nr Var               7217.423    9.882                               \n=============================================================================\n\n```\n\n\n\nThe model summary shows that, assuming an alpha level of .05, there are significant main effects of color type (z = -2.136, p = .033), set size (z = 17.2, p < .001), and a significant color-type by set-size interaction (z = 2.47, p = .014). However, we have selectively analyzed a sample range that we knew, based on a visual inspection of the data, to show these effects. This means that our analysis is circular: we have looked at the data to decide where to look! The `find()` function improves this by splitting the data into training and tests sets, as described under [About](#about), thus breaking the circularity.\n\n\n\n```python\nresults = tst.find(dm, 'pupil ~ set_size * color_type',\n                   groups='subject_nr', winlen=5)\n```\n\n\n\nThe return value of `find()` is a `dict`, where keys are effect labels and values are named tuples of the following:\n\n- `model`: a model as returned by `mixedlm().fit()`\n- `samples`: a `set` with the sample indices that were used\n- `p`: the p-value from the model\n- `z`: the z-value from the model\n\nThe `summarize()` function is a convenient way to get the results in a human-readable format.\n\n\n\n```python\nprint(tst.summarize(results))\n```\n\n__Output:__\n``` .text\nIntercept was tested at samples {95} \u2192 z = -13.1098, p = 2.892e-39, converged = yes\ncolor_type[T.proto] was tested at samples {160, 170, 175} \u2192 z = -2.0949, p = 0.03618, converged = yes\nset_size was tested at samples {185, 210, 195, 255} \u2192 z = 16.2437, p = 2.475e-59, converged = yes\nset_size:color_type[T.proto] was tested at samples {165, 175} \u2192 z = 2.5767, p = 0.009974, converged = yes\n```\n\n\n\nWe can pass the `results` to `plot()` to visualize the results:\n\n\n\n```python\nplt.clf()\ntst.plot(dm, dv='pupil', hue_factor='set_size', linestyle_factor='color_type',\n         results=results, sampling_freq=100)\nplt.savefig('img/signal-plot-2.png')\n```\n\n\n\n![](https://github.com/smathot/time_series_test/raw/master/img/signal-plot-2.png)\n\n\n## Function reference\n\n## <span style=\"color:purple\">time\\_series\\_test.lmer\\_crossvalidation\\_test</span>_(dm, formula, groups, re\\_formula=None, winlen=1, split=4, split\\_method='interleaved', samples\\_fe=True, samples\\_re=True, localizer\\_re=False, fit\\_method=None, suppress\\_convergence\\_warnings=False, fit\\_kwargs=None, \\*\\*kwargs)_\n\nConducts a single linear mixed effects model to a time series, where the\nto-be-tested samples are determined through crossvalidation.\n\nThis function uses `mixedlm()` from the `statsmodels` package. See the\nstatsmodels documentation for a more detailed explanation of the\nparameters.\n\n### Parameters\n\n* **dm: DataMatrix**\n\n  The dataset\n\n* **formula: str**\n\n  A formula that describes the dependent variable, which should be the\n  name of a series column in `dm`, and the fixed effects, which should\n  be regular (non-series) columns.\n\n* **groups: str or None or list of str**\n\n  The groups for the random effects, which should be regular (non-series)\n  columns in `dm`. If `None` is specified, then all analyses are based\n  on a regular multiple linear regression (instead of linear mixed \n  effects model).\n\n* **re\\_formula: str or None**\n\n  A formula that describes the random effects, which should be regular\n  (non-series) columns in `dm`.\n\n* **winlen: int, optional**\n\n  The number of samples that should be analyzed together, i.e. a \n  downsampling window to speed up the analysis.\n\n* **split: int, optional**\n\n  The number of splits that the analysis should be based on.\n\n* **split\\_method: str, optional**\n\n  If 'interleaved', the data is split in a regular interleaved fashion,\n  such that the first row goes to the first subset, the second row to the\n  second subset, etc. If 'random', the data is split randomly in subsets.\n  Interleaved splitting is deterministic (i.e. it results in the same\n  outcome each time), but random splitting is not.\n\n* **samples\\_fe: bool, optional**\n\n  Indicates whether sample indices are included as an additive factor\n  to the fixed-effects formula. If all splits yielded the same sample\n  index, this is ignored.\n\n* **samples\\_re: bool, optional**\n\n  Indicates whether sample indices are included as an additive factor\n  to the random-effects formula. If all splits yielded the same sample\n  index, this is ignored.\n\n* **localizer\\_re: bool, optional**\n\n  Indicates whether a random effects structure as specified using the\n  `re_formula` keyword should also be used for the localizer models,\n  or only for the final model.\n\n* **fit\\_kwargs: dict or None, optional**\n\n  A `dict` that is passed as keyword arguments to `mixedlm.fit()`. For\n  example, to specify the nm as the fitting method, specify\n  `fit_kwargs={'fit': 'nm'}`.\n\n* **fit\\_method: str, list of str, or None, optional**\n\n  Deprecated. Use `fit_kwargs` instead.\n\n* **suppress\\_convergence\\_warnings: bool, optional**\n\n  Installs a warning filter to suppress conververgence (and other)\n  warnings.\n\n* **\\*\\*kwargs: dict, optional**\n\n  Optional keywords to be passed to `mixedlm()`.\n\n### Returns\n\n* **_dict_**\n\n  A dict where keys are effect labels, and values are named tuples\n  of `model`, `samples`, `p`, and `z`.\n\n## <span style=\"color:purple\">time\\_series\\_test.lmer\\_permutation\\_test</span>_(dm, formula, groups, re\\_formula=None, winlen=1, suppress\\_convergence\\_warnings=False, fit\\_kwargs={}, iterations=1000, cluster\\_p\\_threshold=0.05, test\\_intercept=False, \\*\\*kwargs)_\n\nPerforms a cluster-based permutation test based on sample-by-sample\nlinear-mixed-effects analyses. The permutation test identifies clusters\nbased on p-value threshold and uses the absolute of the summed z-values of\nthe clusters as test statistic.\n\nIf no clusters reach the threshold, the test is skipped right away. By\ndefault the Intercept is ignored for this criterion, because the intercept\nusually has significant clusters that we're not interested in. However, you\ncan change this using the `test_intercept` keyword.\n\n*Warning:* This is generally an extremely time-consuming analysis because\nit requires thousands of lmers to be run.\n\nSee `lmer_crossvalidation()` for an explanation of the arguments.\n\n### Parameters\n\n* **dm: DataMatrix**\n\n* **formula: str**\n\n* **groups: str**\n\n* **re\\_formula: str or None, optional**\n\n* **winlen: int, optional**\n\n* **suppress\\_convergence\\_warnings: bool, optional**\n\n* **fit\\_kwargs: dict, optional**\n\n* **iterations: int, optional**\n\n  The number of permutations to run.\n\n* **cluster\\_p\\_threshold: float or None, optional**\n\n  The maximum p-value for a sample to be considered part of a cluster.\n\n* **test\\_intercept: bool, optional**\n\n  Indicates whether the intercept should be included when considering if\n  there are any clusters, as described above.\n\n* **\\*\\*kwargs: dict, optional**\n\n### Returns\n\n* **_dict_**\n\n  A dict with effects as keys and lists of clusters defined by\n  (start, end, z-sum, hit proportion) tuples. The p-value is\n  1 - hit proportion.\n\n## <span style=\"color:purple\">time\\_series\\_test.lmer\\_series</span>_(dm, formula, winlen=1, fit\\_kwargs={}, \\*\\*kwargs)_\n\nPerforms a sample-by-sample linear-mixed-effects analysis. See\n`lmer_crossvalidation()` for an explanation of the arguments.\n\n### Parameters\n\n* **dm: DataMatrix**\n\n* **formula: str**\n\n* **winlen: int, optional**\n\n* **fit\\_kwargs: dict, optional**\n\n* **\\*\\*kwargs: dict, optional**\n\n### Returns\n\n* **_DataMatrix_**\n\n  A DataMatrix with one row per effect, including the intercept, and\n  three series columns with the same depth as the dependent measure\n  specified in the formula:\n\n  - `est`: the slope\n  - `p`: the p value\n  - `z`: the z value\n  - `se`: the standard error\n\n## <span style=\"color:purple\">time\\_series\\_test.plot</span>_(dm, dv, hue\\_factor, results=None, linestyle\\_factor=None, hues=None, linestyles=None, alpha\\_level=0.05, annotate\\_intercept=False, annotation\\_hues=None, annotation\\_linestyle=':', legend\\_kwargs=None, annotation\\_legend\\_kwargs=None, x0=0, sampling\\_freq=1)_\n\nVisualizes a time series, where the signal is plotted as a function of\nsample number on the x-axis. One fixed effect is indicated by the hue\n(color) of the lines. An optional second fixed effect is indicated by the\nlinestyle. If the `results` parameter is used, significant effects are\nannotated in the figure.\n\n### Parameters\n\n* **dm: DataMatrix**\n\n  The dataset\n\n* **dv: str**\n\n  The name of the dependent variable, which should be a series column\n  in `dm`.\n\n* **hue\\_factor: str**\n\n  The name of a regular (non-series) column in `dm` that specifies the\n  hue (color) of the lines.\n\n* **results: dict, optional**\n\n  A `results` dict as returned by `lmer_crossvalidation()`.\n\n* **linestyle\\_factor: str, optional**\n\n  The name of a regular (non-series) column in `dm` that specifies the\n  linestyle of the lines for a two-factor plot.\n\n* **hues: str, list, or None, optional**\n\n  The name of a matplotlib colormap or a list of hues to be used as line\n  colors for the hue factor.\n\n* **linestyles: list or None, optional**\n\n  A list of linestyles to be used for the second factor.\n\n* **alpha\\_level: float, optional**\n\n  The alpha level (maximum p value) to be used for annotating effects\n  in the plot.\n\n* **annotate\\_intercept: bool, optional**\n\n  Specifies whether the intercept should also be annotated along with\n  the fixed effects.\n\n* **annotation\\_hues: str, list, or None, optional**\n\n  The name of a matplotlib colormap or a list of hues to be used for the\n  annotations if `results` is provided.\n\n* **annotation\\_linestyle: str, optional**\n\n  The linestyle for the annotations.\n\n* **legend\\_kwargs: None or dict, optional**\n\n  Optional keywords to be passed to `plt.legend()` for the factor legend.\n\n* **annotation\\_legend\\_kwargs: None or dict, optional**\n\n  Optional keywords to be passed to `plt.legend()` for the annotation\n  legend.\n\n* **x0: int, float**\n\n  The starting value on the x-axis.\n\n* **sampling\\_freq: int, float**\n\n  The sampling frequency.\n\n## <span style=\"color:purple\">time\\_series\\_test.summarize</span>_(results, detailed=False)_\n\nGenerates a string with a human-readable summary of a results `dict` as\nreturned by `lmer_crossvalidation()`.\n\n### Parameters\n\n* **results: dict**\n\n  A `results` dict as returned by `lmer_crossvalidation()`.\n\n* **detailed: bool, optional**\n\n  Indicates whether model details should be included in the summary.\n\n### Returns\n\n* **_str_**\n\n\n## License\n\n`time_series_test` is licensed under the [GNU General Public License\nv3](http://www.gnu.org/licenses/gpl-3.0.en.html).\n",
    "url": "https://github.com/smathot/time_series_test",
    "collection": "datamatrix",
    "howto": false,
    "foundation": false,
    "topics": [
      "datamatrix"
    ]
  },
  {
    "content": "# How to combine multiple datasets into a single DataMatrix\n\nWhen you have data from multiple experiments or sessions stored in separate folders, you often need to combine them into a single DataMatrix for analysis. This howto shows how to load multiple datasets and merge them while keeping track of their origin.\n\n```python\nfrom datamatrix import DataMatrix\n\n# Load data from different experiments/folders\ndm1 = get_dm('data_1')\ndm1.subject_nr += 1000  # Add offset to distinguish experiments\ndm2 = get_dm('data_2')\ndm2.subject_nr += 2000  # Different offset for experiment 2\ndm3 = get_dm('data_3')\ndm3.subject_nr += 3000  # Different offset for experiment 3\n\n# Combine all datamatrices using the << operator\ndm = dm1 << dm2 << dm3\n```",
    "title": "How to combine multiple datasets into a single DataMatrix",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to filter out missing or invalid data\n\nReal-world data often contains missing values or invalid entries. This howto demonstrates how to remove rows with missing data using DataMatrix's intuitive selection syntax.\n\n```python\n# Remove rows where important columns have missing values\ndm = dm.correct != ''  # Remove empty strings\ndm = dm.cue_eccentricity != ''\ndm = dm.response_time != ''\n\n# Remove rows where values are not of expected type\ndm = dm.subject_nr == int  # Keep only integer subject numbers\n\n# You can also remove specific values\ndm = dm.practice != 'yes'  # Remove practice trials\n```",
    "title": "How to filter out missing or invalid data",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to calculate z-scores per participant\n\nWhen analyzing data from multiple participants, you often need to standardize values within each participant. This howto shows how to calculate z-scores separately for each participant.\n\n```python\nfrom datamatrix import operations as ops\n\n# Initialize a column for z-scored values\ndm.z_baseline = ''\n\n# Loop through participants and calculate z-scores\nfor subject_nr, sdm in ops.split(dm.subject_nr):\n    # Calculate z-scores for this participant's data\n    dm.z_baseline[sdm] = ops.z(sdm.baseline)\n```",
    "title": "How to calculate z-scores per participant",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to remove outliers based on z-scores\n\nAfter calculating z-scores, you typically want to remove extreme outliers. This howto demonstrates how to filter data based on z-score thresholds.\n\n```python\n# Remove outliers beyond 2 standard deviations\nprint(f'Before removing outliers: N(trial) = {len(dm)}')\ndm = dm.z_baseline >= -2\ndm = dm.z_baseline <= 2\nprint(f'After removing outliers: N(trial) = {len(dm)}')\n```",
    "title": "How to remove outliers based on z-scores",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to work with time series data\n\nDataMatrix supports time series data through SeriesColumn objects. This howto shows how to extract specific time windows and calculate means across time points.\n\n```python\nfrom datamatrix import series as srs, NAN\n\n# Extract a specific time window (e.g., samples 75-300)\ndm.pupil_window = dm.pupil_stream[:, 75:300]\n\n# Calculate mean across a time window (e.g., samples 250-260)\ndm.mean_pupil = srs.reduce(dm.pupil_stream[:, 250:260])\n\n# Remove rows where the mean is NaN\ndm_valid = dm.mean_pupil != NAN\n```",
    "title": "How to work with time series data",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to split data by experimental conditions\n\nExperimental data often needs to be analyzed separately for different conditions. This howto demonstrates how to split a DataMatrix based on condition values.\n\n```python\nfrom datamatrix import operations as ops\n\n# Split by a single condition with automatic detection of values\nfor group, group_dm in ops.split(dm.condition):\n    print(f'Analyzing condition: {group}')\n    # Perform analysis on group_dm\n\n# Split by specific values in a specific order\ndm_cond1, dm_cond2 = ops.split(dm.condition, 'eccentricity', 'location')\n```",
    "title": "How to split data by experimental conditions",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to get summary statistics by group\n\nWhen analyzing grouped data, you often need summary statistics for each group. This howto shows how to iterate through groups and calculate statistics.\n\n```python\nfrom datamatrix import operations as ops\n\n# Calculate mean for each group\nfor group, group_dm in ops.split(dm.Group):\n    print(f'Mean heart rate for {group} is {group_dm[\"Heart Rate\"].mean} bpm')\n```",
    "title": "How to get summary statistics by group",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to count unique values and participants\n\nUnderstanding your data structure often requires counting unique values. This howto demonstrates how to count participants and other categorical variables.\n\n```python\n# Count number of unique participants\nn_participants = dm.subject_nr.count\nprint(f'Number of participants: {n_participants}')\n\n# Check specific count\nassert dm.subject_nr.count == 100\n\n# Get list of unique values\nunique_conditions = dm.condition.unique\nprint(f'Conditions: {unique_conditions}')\n```",
    "title": "How to count unique values and participants",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to perform row-wise and column-wise iterations\n\nDataMatrix provides multiple ways to iterate through your data. This howto shows different iteration patterns for various use cases.\n\n```python\n# Iterate through rows\nfor row in dm:\n    print(f'Subject {row.subject_nr}: RT = {row.response_time}')\n    \n# Iterate through columns\nfor colname, col in dm.columns:\n    if col.mean is not None:  # Check if column is numeric\n        print(f'{colname}: M = {col.mean:.2f}, SD = {col.std:.2f}')\n\n# Iterate through cells in a specific column\nfor cell in dm.response_time:\n    if cell > 1000:  # Flag slow responses\n        print(f'Slow response: {cell} ms')\n```",
    "title": "How to perform row-wise and column-wise iterations",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to create new columns based on calculations\n\nCreating derived variables is a common task in data analysis. This howto demonstrates various ways to create new columns from existing data.\n\n```python\n# Simple arithmetic operations\ndm.rt_seconds = dm.response_time / 1000  # Convert ms to seconds\ndm.double_score = dm.score * 2\n\n# Conditional assignment\ndm.accuracy_category = ''\ndm.accuracy_category[dm.correct == 1] = 'correct'\ndm.accuracy_category[dm.correct == 0] = 'incorrect'\n\n# Using functions with the @ operator\ndm.log_rt = dm.response_time @ np.log\n```",
    "title": "How to create new columns based on calculations",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to handle mixed data types in statistical analyses\n\nWhen running statistical analyses, you need to ensure your data is properly formatted. This howto shows how to prepare data for mixed-effects models and GLMs.\n\n```python\nfrom datamatrix import NAN\nfrom statsmodels.formula.api import mixedlm\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\n# For mixed-effects models - ensure no NaN values\ndm_valid = dm.dependent_variable != NAN\n\n# Run mixed-effects model\nmodel = mixedlm(\n    formula='dependent_variable ~ predictor1 * predictor2',\n    data=dm_valid,\n    groups='subject_nr'\n).fit()\nprint(model.summary())\n\n# For logistic regression with binary outcomes\nglm_model = smf.glm(\n    formula='correct ~ condition',\n    data=dm,\n    family=sm.families.Binomial()\n).fit()\nprint(glm_model.summary())\n```",
    "title": "How to handle mixed data types in statistical analyses",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to perform cross-validation for time-series analysis\n\nWhen analyzing time-series data, cross-validation helps identify significant time windows. This howto demonstrates how to use the find() function for temporal analysis.\n\n```python\nimport time_series_test as tst\n\n# Prepare time window\ndm.pupil_window = dm.pupil_stream[:, 75:300]\n\n# Run cross-validation analysis\nresults = tst.lmer_crossvalidation_test(\n    dm, \n    'pupil_window ~ predictor',  # Formula\n    groups='subject_nr',          # Group by participant\n    winlen=5,                     # Window length\n    suppress_convergence_warnings=True\n)\n\n# Summarize results\nprint(tst.summarize(results))\n```",
    "title": "How to perform cross-validation for time-series analysis",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to filter rows based on column values\n\nWhen analyzing data, you often need to select only rows that meet certain criteria. DataMatrix allows you to filter data using intuitive comparison operators, which is useful for excluding outliers, selecting specific conditions, or focusing on subsets of your data.\n\n```python\nfrom datamatrix import DataMatrix\n\n# Create a sample dataset\ndm = DataMatrix(length=10)\ndm.accuracy = .7, .8, .65, .9, .55, .95, .72, .88, .6, .85\ndm.condition = 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'\n\n# Select only rows where accuracy is above 0.7\ndm_high_accuracy = dm.accuracy > 0.7\nprint(f\"Rows with high accuracy: {len(dm_high_accuracy)}\")\n\n# Combine multiple conditions using & (and), | (or)\ndm_filtered = (dm.accuracy > 0.7) & (dm.condition == 'B')\nprint(f\"High accuracy in condition B: {len(dm_filtered)}\")\n\n# Select rows where accuracy is NOT equal to a value\ndm_not_equal = dm.accuracy != 0.6\n```",
    "title": "How to filter rows based on column values",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to split data into groups\n\nSplitting data by categorical variables is essential for comparing groups or analyzing data separately by condition. DataMatrix provides the `ops.split()` function to easily divide your data based on column values, which is particularly useful for between-group comparisons.\n\n```python\nfrom datamatrix import DataMatrix, operations as ops\n\n# Create sample data with groups\ndm = DataMatrix(length=9)\ndm.group = 'control', 'control', 'control', 'treatment1', 'treatment1', 'treatment1', 'treatment2', 'treatment2', 'treatment2'\ndm.score = 10, 12, 11, 15, 16, 14, 20, 22, 21\n\n# Split into separate DataMatrix objects for each group\ndm_control, dm_treatment1, dm_treatment2 = ops.split(dm.group, 'control', 'treatment1', 'treatment2')\n\nprint(f\"Control group mean: {dm_control.score.mean}\")\nprint(f\"Treatment1 group mean: {dm_treatment1.score.mean}\")\nprint(f\"Treatment2 group mean: {dm_treatment2.score.mean}\")\n\n# Alternatively, iterate through groups when you don't know the values in advance\nfor group_name, group_dm in ops.split(dm.group):\n    print(f\"{group_name}: n={len(group_dm)}, mean={group_dm.score.mean}\")\n```",
    "title": "How to split data into groups",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to create new columns based on conditions\n\nOften you need to create derived variables or recode existing data. DataMatrix allows you to create new columns and set their values based on conditions, which is useful for creating categorical variables from continuous ones or coding experimental conditions.\n\n```python\nfrom datamatrix import DataMatrix\n\ndm = DataMatrix(length=6)\ndm.response_time = 250, 450, 320, 680, 510, 390\n\n# Create a categorical column based on response time\ndm.speed_category = 'slow'  # Initialize all as 'slow'\ndm.speed_category[dm.response_time < 400] = 'fast'\ndm.speed_category[dm.response_time > 600] = 'very_slow'\n\n# Create a numeric coding based on multiple columns\ndm.condition_type = 'A', 'B', 'A', 'B', 'A', 'B'\ndm.difficulty = 'easy', 'easy', 'hard', 'hard', 'easy', 'hard'\n\n# Create numeric codes for analysis\ndm.condition_code = 0\ndm.condition_code[dm.condition_type == 'A'] = 1\ndm.condition_code[dm.condition_type == 'B'] = 2\n# Add extra coding for difficulty\ndm.condition_code[dm.difficulty == 'hard'] += 10\n```",
    "title": "How to create new columns based on conditions",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to calculate group statistics\n\nAggregating data by groups is a common requirement in data analysis. DataMatrix provides the `ops.group()` function to calculate statistics for each group, which is useful for creating summary tables or preparing data for statistical tests.\n\n```python\nfrom datamatrix import DataMatrix, operations as ops\nfrom datamatrix import functional as fnc\n\n# Create sample data\ndm = DataMatrix(length=12)\ndm.subject = 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4\ndm.condition = 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'\ndm.score = 10, 12, 15, 11, 13, 16, 9, 11, 14, 12, 14, 17\n\n# Group by subject to get mean score per subject\ngrouped_by_subject = ops.group(dm, by=dm.subject)\nprint(f\"Grouped data has {len(grouped_by_subject)} subjects\")\n\n# Group by multiple columns\ngrouped = ops.group(dm, by=[dm.subject, dm.condition])\n# Note: grouped data will have list-like cells that need to be reduced\nprint(f\"Each cell contains: {grouped.score[0]}\")  # Shows list of values\n\n# Calculate mean for each group using a custom function\ndef mean_of_list(lst):\n    return sum(lst) / len(lst) if len(lst) > 0 else 0\n\ngrouped.mean_score = grouped.score @ mean_of_list\n```",
    "title": "How to calculate group statistics",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to handle missing data\n\nReal-world data often contains missing values. DataMatrix uses `NAN` to represent missing data and provides ways to filter out or handle these values, which is crucial for ensuring your analyses aren't corrupted by missing data.\n\n```python\nfrom datamatrix import DataMatrix, NAN\n\n# Create data with missing values\ndm = DataMatrix(length=6)\ndm.value = 10, NAN, 25, 30, NAN, 45\n\n# Check which rows have valid (non-NAN) data\ndm_valid = dm.value != NAN\nprint(f\"Valid rows: {len(dm_valid)} out of {len(dm)}\")\n\n# Calculate mean (automatically ignores NAN)\nprint(f\"Mean of values: {dm.value.mean}\")\n\n# Count non-NAN values in a column\nimport numpy as np\ndef count_valid(col):\n    return np.sum([v == v for v in col])  # NaN != NaN in Python\n\nn_valid = count_valid(dm.value)\nprint(f\"Number of valid values: {n_valid}\")\n\n# Fill missing values with a default\ndm.value[dm.value != dm.value] = 0  # Replace NaN with 0\n```",
    "title": "How to handle missing data",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to apply functions to columns\n\nTransforming data often requires applying functions to entire columns. DataMatrix provides the `@` operator for applying functions element-wise, which is useful for data transformations, standardization, or creating derived variables.\n\n```python\nfrom datamatrix import DataMatrix\nimport numpy as np\n\ndm = DataMatrix(length=5)\ndm.raw_score = 10, 20, 30, 40, 50\n\n# Apply a simple transformation\ndm.log_score = dm.raw_score @ np.log10\n\n# Apply a custom function\ndef standardize(x, mean=30, sd=10):\n    return (x - mean) / sd\n\ndm.z_score = dm.raw_score @ (lambda x: standardize(x))\n\n# Apply more complex transformations\ndef categorize(x):\n    if x < 20:\n        return 'low'\n    elif x < 40:\n        return 'medium'\n    else:\n        return 'high'\n\ndm.category = dm.raw_score @ categorize\n```",
    "title": "How to apply functions to columns",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to work with time series data\n\nTime series data requires special handling. DataMatrix supports SeriesColumn objects for time-series data, with built-in functions for baseline correction, averaging, and time-window selection, making it ideal for analyzing temporal data like EEG, pupillometry, or reaction times over trials.\n\n```python\nfrom datamatrix import DataMatrix, SeriesColumn\nfrom datamatrix import series as srs\nimport numpy as np\n\n# Create time series data\ndm = DataMatrix(length=3)\ndm.pupil = SeriesColumn(depth=100)  # 100 time points\n\n# Simulate some data\nfor i, row in enumerate(dm):\n    # Create different patterns for each row\n    baseline = i * 0.1\n    signal = np.sin(np.linspace(0, 2*np.pi, 100)) * (i + 1)\n    row.pupil = baseline + signal + np.random.normal(0, 0.1, 100)\n\n# Baseline correction (subtract mean of first 10 samples)\ndm.pupil_baseline = srs.baseline(dm.pupil, dm.pupil, 0, 10)\n\n# Select a time window (samples 20-50)\ndm.response_period = dm.pupil[:, 20:50]\n\n# Reduce time series to single value (mean)\ndm.mean_response = srs.reduce(dm.response_period)\n\n# Apply function to each time point\ndm.abs_pupil = dm.pupil @ np.abs\n```",
    "title": "How to work with time series data",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to iterate over rows\n\nIterating through individual observations is common when you need row-wise operations or complex conditional logic. DataMatrix provides intuitive row iteration that gives you access to all columns for each observation.\n\n```python\nfrom datamatrix import DataMatrix\n\ndm = DataMatrix(length=4)\ndm.subject_id = 'S01', 'S02', 'S03', 'S04'\ndm.accuracy = 0.8, 0.75, 0.9, 0.85\ndm.rt = 450, 520, 380, 410\n\n# Simple iteration to process each row\nfor row in dm:\n    print(f\"Subject {row.subject_id}: accuracy={row.accuracy}, RT={row.rt}\")\n    \n# Create new column based on row-wise calculation\nfor row in dm:\n    # Calculate efficiency score (accuracy/time)\n    row.efficiency = row.accuracy / (row.rt / 1000)\n    \n# Conditional processing based on multiple columns\nfor row in dm:\n    if row.accuracy > 0.8 and row.rt < 450:\n        row.performance = 'excellent'\n    elif row.accuracy > 0.7:\n        row.performance = 'good'\n    else:\n        row.performance = 'needs improvement'\n```",
    "title": "How to iterate over rows",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to rename columns\n\nColumn names often need to be changed for clarity or compatibility. DataMatrix provides a simple `rename()` method for updating column names, which is useful when importing data with unclear names or preparing data for specific analysis packages.\n\n```python\nfrom datamatrix import DataMatrix, io\n\n# Load data that might have unclear column names\ndm = DataMatrix(length=5)\ndm.var1 = 10, 20, 30, 40, 50\ndm.var2 = 'A', 'B', 'A', 'B', 'A'\ndm.var3 = 1.1, 2.2, 3.3, 4.4, 5.5\n\n# Rename single column\ndm.rename('var1', 'response_time')\ndm.rename('var2', 'condition')\ndm.rename('var3', 'accuracy')\n\n# Check column names\nprint(f\"Column names: {dm.column_names}\")\n\n# Alternative: access columns with special characters or spaces\ndm['response time (ms)'] = dm.response_time\ndel dm.response_time  # Remove old column\n```",
    "title": "How to rename columns",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to combine DataMatrix objects\n\nCombining data from multiple sources is often necessary. DataMatrix can be combined row-wise (concatenation) or column-wise (merging), which is useful for combining datasets from different experimental sessions or adding calculated variables.\n\n```python\nfrom datamatrix import DataMatrix, operations as ops\n\n# Create two datasets to combine\ndm1 = DataMatrix(length=3)\ndm1.subject = 1, 2, 3\ndm1.session = 1, 1, 1\ndm1.score = 10, 15, 12\n\ndm2 = DataMatrix(length=3)\ndm2.subject = 4, 5, 6\ndm2.session = 2, 2, 2\ndm2.score = 11, 16, 13\n\n# Combine row-wise (concatenate)\ndm_combined = dm1 << dm2\nprint(f\"Combined length: {len(dm_combined)}\")\n\n# Add columns from another DataMatrix (must have same length)\ndm_extra = DataMatrix(length=6)\ndm_extra.age = 20, 22, 21, 23, 19, 24\n\n# Copy columns\ndm_combined.age = dm_extra.age\n```",
    "title": "How to combine DataMatrix objects",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to save and load data\n\nPersisting your processed data is essential for reproducibility. DataMatrix supports multiple file formats, with CSV and Excel being the most common, allowing easy data exchange with other tools and collaborators.\n\n```python\nfrom datamatrix import DataMatrix, io\n\n# Create sample data\ndm = DataMatrix(length=5)\ndm.participant = 'P01', 'P02', 'P03', 'P04', 'P05'\ndm.age = 22, 25, 21, 28, 24\ndm.score = 85.5, 90.2, 78.3, 92.1, 88.7\n\n# Save as CSV (readable by any software)\nio.writetxt(dm, 'my_data.csv')\n\n# Save as Excel\nio.writexlsx(dm, 'my_data.xlsx')\n\n# Load data back\ndm_loaded = io.readtxt('my_data.csv')\n# or\ndm_loaded = io.readxlsx('my_data.xlsx')\n\n# For large datasets, you might want to select columns when loading\ndm_subset = io.readtxt('my_data.csv')\ndm_subset = dm_subset[dm_subset.participant, dm_subset.score]  # Keep only specific columns\n```",
    "title": "How to save and load data",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  },
  {
    "content": "# How to perform element-wise operations\n\nMathematical operations on columns are fundamental for data transformation. DataMatrix supports element-wise operations that automatically handle missing values, making it easy to create derived variables or normalize data.\n\n```python\nfrom datamatrix import DataMatrix\n\ndm = DataMatrix(length=5)\ndm.x = 10, 20, 30, 40, 50\ndm.y = 2, 4, 6, 8, 10\n\n# Basic arithmetic operations\ndm.sum = dm.x + dm.y\ndm.difference = dm.x - dm.y\ndm.product = dm.x * dm.y\ndm.ratio = dm.x / dm.y\n\n# Operations with constants\ndm.x_doubled = dm.x * 2\ndm.x_centered = dm.x - dm.x.mean\ndm.x_scaled = dm.x / dm.x.std\n\n# Combining operations\ndm.normalized = (dm.x - dm.x.mean) / dm.x.std\n\n# String concatenation also works with +\ndm.name = 'Subject_', 'Subject_', 'Subject_', 'Subject_', 'Subject_'\ndm.id = '001', '002', '003', '004', '005'\ndm.full_id = dm.name + dm.id\n```",
    "title": "How to perform element-wise operations",
    "topics": [
      "datamatrix"
    ],
    "collection": "datamatrix",
    "howto": true,
    "source": "howtos",
    "foundation": false
  }
]